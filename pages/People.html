<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Deng-Ping Fan&#39;s home page">
    <link rel="shortcut icon" href="./images/logo-ethz2.png">
    <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://dengpingfan.github.io/assets/jemdoc.css" type="text/css">

    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88572407-1', 'auto');
      ga('send', 'pageview');
    </script>
    <meta name="google-site-verification" content="F0Q0t5oLq1pGwXGMf_38oA2MxW_zfiMRsQTYD4_GJoQ"/>
    <title>Deng-Ping Fan</title>
  </head>

  <body>
    
    <div id="layout-content" style="margin-top:25px">
      <table>
        <tbody>
          <tr>
            <td width="670">
              <div id="toptitle">
                <h1>Deng-Ping Fan &nbsp; 范登平</h1>
              </div>
              
              <div class="navbar-collapse collapse">
                <h2>
                <a href="https://dengpingfan.github.io/index.html">Home|</a>
                <a href="People.html">People|</a>
                <a href="Publication.html">Publications|</a>
                <a href="Services.html">Services|</a>  
                <a href="Accept.html">AcceptRate</a>   
               </h2>
            </div>    
              <p>
                <a href="https://vision.ee.ethz.ch/the-institute.html">Computer Vision Lab</a>, ETH Zürich<br>
                ETF C113.2, Sternwartstrasse 7, 8092 Zürich, Switzerland<br>
                Email: dengpfan(AT)gmail(DOT)com<br>
              </p>
              <h3 style="padding-top:-5px"></h3>
              <object id="object" data="https://dengpingfan.github.io/assets/scholar.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=zh-CN&oi=ao" target="_blank">Google Scholar</a>
              &nbsp; &nbsp; &nbsp;
              <img src="https://dengpingfan.github.io/assets/SeSc.png" width="15" height="15" style="margin-bottom:-3px">&nbsp;
              <a href="https://www.semanticscholar.org/author/Deng-Ping-Fan/23999143">Semantic Scholar</a>
              &nbsp; &nbsp; &nbsp;
              <img src="https://dengpingfan.github.io/assets/RG.png" width="15" height="15" style="margin-bottom:-3px">&nbsp;
              <a href="https://www.researchgate.net/profile/Deng-Ping-Fan">Research Gate</a>
              &nbsp; &nbsp; &nbsp;
              <img src="https://dengpingfan.github.io/assets/dblp.png" width="15" height="15" style="margin-bottom:-3px">&nbsp;
              <a href="https://dblp.org/pid/205/3148.html">DBLP</a>
              &nbsp; &nbsp; &nbsp;
              <object id="object" data="https://dengpingfan.github.io/assets/AMiner.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://www.aminer.cn/ai2000/search_rank?id=53f31e7fdabfae9a844480c5&searchValue=%E8%8C%83%E7%99%BB%E5%B9%B3&yearLeft=2020&yearRight=2023" target="_blank">AI Open Index</a>
              &nbsp; &nbsp; &nbsp;
              <object id="object" data="https://dengpingfan.github.io/assets/github.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://github.com/DengPingFan" target="_blank">Github</a> 
              &nbsp; &nbsp; &nbsp;
              <img src="https://dengpingfan.github.io/assets/linkin.png" width="15" height="15" style="margin-bottom:-3px">&nbsp;
              <a href="https://www.linkedin.com/in/deng-ping-fan-584b25198/">Linkedin</a>
              &nbsp; &nbsp; &nbsp;
              <a href="http://web.archive.org/web/20220112230725/http://dpfan.net/">Previous-Homepage</a>
            </td>
            <td>
              <img src="https://dengpingfan.github.io/images/2-inch.jpg" border="0" width="150">
            </td>
          </tr>
        </tbody>
      </table>
      
      <!-- Biography -->
      <h2> 
        <a id="Biography" class="anchor" href="#Biography">Biography</a> 
      </h2>
      <ul>
         <p>
            Deng-Ping Fan is a Researcher, working with <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html"><u>Prof. Luc Van Gool</u></a> in <a href="https://vision.ee.ethz.ch/"><u>Computer Vision Lab</u></a> @ <a href="https://ethz.ch/en.html"><u>ETH Zurich</u></a>.
            From 2019-2021, he was a research scientist (PI) and team lead of IIAI-CV&Med in <a href="https://www.inceptioniai.org/en"><u>IIAI</u></a>, working with <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN"><u>Prof. Ling Shao</u></a> and <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=zh-CN"><u>Prof. Jianbing Shen</u></a>.
            He received his Ph.D. from Nankai University in 2019 under the supervision of <a href="https://mmcheng.net/cmm/"><u>Prof. Ming-Ming Cheng</u></a>.
            His research interest is in Computer Vision, Machine Learning, and Medical Image Analysis. 
            Specifically, he focuses on <a href="https://xuebinqin.github.io/dis/index.html">Dichotomous Image Segmentation</a> 
            (General Object Segmentation, <a href="http://mmcheng.net/cod/">Camouflaged Object Segmentation</a>, Saliency Detection), 
            Multi-Modal AI. He is a senior member of IEEE and is one of the core technique members in <a href="https://www.trace.ethz.ch/team/index.html">TRACE-Zurich project</a> on automated driving. 
            [<a href="https://www.overleaf.com/read/fhcjfgghdddr">CV</a>]
          </p>

          <p>
            范登平，于2019年以优秀博士生荣誉从南开大学获得博士学位，现为瑞士苏黎世联邦理工大学研究人员，合作导师为Luc Van Gool教授，曾任阿联酋起源人工智能研究院研究员。
            研究方向为计算机视觉和医学图像分析，在SCI一区/CCF A类刊物上发表论文约50篇（2次CVPR最佳论文提名、7篇TPAMI论文），H指数40，论文谷歌学术引用9500次，通讯作者论文单篇最高引用1270次，两次进入斯坦福全球前2%顶尖科学家榜单，获吴文俊人工智能自然科学二等奖(排四)。
            设计的基于组-背景挖掘的搜索识别模型<a href="https://github.com/GewelsJI/SINet-V2">SINet-V2</a>，因大幅提升“<a href="https://ieeexplore.ieee.org/document/9444794"><u>伪装目标检测</u></a>”任务(<a href="http://mc.nankai.edu.cn/cod">Demo</a>)精度而取得了当前国际领先水平，被英国权威杂志《New Scientist》报道。
            应用方面，公开16项<a href="http://www.soopat.com/Home/Result?Sort=&View=&Columns=&Valid=&Embed=&Db=&Ids=&FolderIds=&FolderId=&ImportPatentIndex=&Filter=&SearchWord=FMR%3A%28%E8%8C%83%E7%99%BB%E5%B9%B3%29+&FMZL=Y">中</a>/美发明专利，
            技术被用于华为、阿里 (<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhuge_Kaleido-BERT_Vision-Language_Pre-Training_on_Fashion_Domain_CVPR_2021_paper.html"><u>Kaleido-BERT</u></a>
            被用于阿里电商搜索系统) 、日本LPIXEL公司。设计的并行反向注意力网络<a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26"><u>PraNet</u></a>  
            可从结肠镜图像中精准地分割息肉，为医生辅助诊断提供了非常有价值的信息，在MediaEval2020竞赛上取得精度指标冠军并被<a href="https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf">斯坦福2022年人工智能指数报告引用</a>,
            被集成到华为算法库中(<a href="https://gitee.com/ascend/modelzoo">Ascend ModelZoo</a>, <a href="https://gitee.com/ascend/ModelZoo-PyTorch/tree/master/PyTorch/contrib/cv/semantic_segmentation/PraNet">link</a>)。            
            主编1本中文学术专著，是IEEE高级会员、CVPR2023领域主席、IJCAI21和23的资深程序委员会委员、CCF-CV执行委员、任中国卓越期刊行动计划国际刊物<a href="https://www.springer.com/journal/11633/editors">MIR</a>编委。
          </p> 
      </ul>
      
      <!-- Research Interests -->
      <h2> 
        <a id="Interests" class="anchor" href="#Interests">Research Interests</a> 
      </h2>
      <ul>
        <li>
           <strong>Multi-Modal AI:</strong> Vision-Language, Vision-Audio, Vision-Language-Audio<br>
        </li>
        <li>
           <strong>Medical Image Segmentation:</strong> Hippocampal segmentation, Polyp Segmentation, Lung Infection Segmentation<br>
        </li>
        <li>
           <strong>3D Vision:</strong> 3D reconstruction, AR/VR, Depth estimation, Light Field SOD (LFSOD), RGB-D SOD<br>
        </li>
        <li>
           <strong>Visual Attention:</strong> Image Salient Object Detection (ISOD), RGB-D SOD, RGB-T SOD, Remote Sensing SOD, High-Resolution SOD (HRSOD), Lidar/Point Cloud SOD, CoSOD, Salient Instance Detection (SID), Salient Object Ranking (SOR), Video Salient Object Detection (VSOD), Visual-auditory SOD<br>
        </li>
        <li>
           <strong>Camouflaged Scene Understanding:</strong> Camouflaged Object Detection (<a href="COD.html">COD</a>), Video Camouflaged Object Detection (VCOD), Camouflaged Instance Segmentation (CIS), Out-Of-Distribution (OOD), Transparent Object Detection, Defect Detection, Abnormal Detection<br>
        </li>
      </ul>

      <!-- Research Experiences -->
      <h2> 
        <a id="Experiences" class="anchor" href="#Experiences">Research Experiences</a> 
      </h2>
      <ul>
       <li>
          <p>
            <a href="https://vision.ee.ethz.ch/the-institute.html">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zürich</a>, Zürich, Switzerland <br>
            Researcher, Oct. 2021 ~ Now <br>
            Supervisor: Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a><br>
          </p>
       </li>
       <li>
          <p>
            <a href="http://www.inceptioniai.org/en/">IIAI</a>, Abu Dhabi, UAE<br>
            Research Scientist, 2019 ~ 2021 <br>
            Working with Prof. <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN">Ling Shao</a> and Prof. <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=zh-CN">Jianbing Shen</a> <br>
          </p>
       </li>
       <li>
          <p>
            <a href="https://mmcheng.net/cmm/">Media Computing Lab</a>, Nankai University, China<br>
            Research Assistant, 2015 ~ 2019 <br>
            Supervisor: Prof. <a href="https://scholar.google.com/citations?user=huWpVyEAAAAJ&hl=zh-CN">Ming-Ming Cheng</a>, Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=9qm5OcYAAAAJ">Bo Ren</a>, Prof. <a href="https://cc.nankai.edu.cn/2022/0602/c13525a455840/page.htm">Xiaoguang Liu</a><br>
          </p>
       </li>
       <!--<li>
          <p>
            <a href="http://www.inceptioniai.org/en/">IIAI</a>, Abu Dhabi, UAE<br>
            Research Scientist & IIAI-CV/Med Team Lead, Jun. 2021 ~ Oct. 2021 <br>
            Working with Prof. <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN">Ling Shao</a> <br>
          </p>
       </li> 
       <li>
          <p>
            <a href="http://www.inceptioniai.org/en/">IIAI</a>, Abu Dhabi, UAE<br>
            Research Scientist, Oct. 2020 ~ Jun. 2021 <br>
            Working with Prof. <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN">Ling Shao</a> and Prof. <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=zh-CN">Jianbing Shen</a> <br>
          </p>
       </li> 
       <li>
          <p>
            <a href="http://www.inceptioniai.org/en/">IIAI</a>, Abu Dhabi, UAE<br>
            Research Associate, Jul. 2019 ~ Oct. 2020 <br>
            Working with Prof. <a href="https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=zh-CN">Jianbing Shen</a> <br>
          </p>
       </li>
       <li>
          <p>
            <a href="https://damo.alibaba.com/">Alibaba Group</a>, Hangzhou, China<br>
            Senior Algorithm Engineer, May 2019 ~ Jul. 2019 <br>
            Supervisor: <a href="https://scholar.google.com/citations?hl=zh-CN&user=NpTmcKEAAAAJ">Ying Chen</a>, Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=9RxI7UAAAAAJ">Yangyan Li</a><br>
          </p>
       </li>
       <li>
          <p>
            <a href="https://mmcheng.net/cmm/">Media Computing Lab</a>, Nankai University, China<br>
            Research Assistant, Sep. 2015 ~ Jun. 2019 <br>
            Supervisor: Prof. <a href="https://scholar.google.com/citations?user=huWpVyEAAAAJ&hl=zh-CN">Ming-Ming Cheng</a>, Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=9qm5OcYAAAAJ">Bo Ren</a>, Prof. <a href="https://cc.nankai.edu.cn/2022/0602/c13525a455840/page.htm">Xiaoguang Liu</a><br>
          </p>
       </li>
       <li>
          <p>
            Guangxi Normal University, China<br>
            Research Assistant, Sep. 2012 ~ Jun. 2015 <br>
            Supervisor: Prof. <a href="http://www.cs.gxnu.edu.cn/2019/0223/c4862a142523/page.htm">Guang-Hai Liu</a>, Prof. <a href="http://www.cs.gxnu.edu.cn/2015/0915/c4857a94375/page.htm">Xiaojian Li</a> <br>
          </p>
       </li>
       <li>
          <p>
            Fujian Agriculture and Forestry University, China<br>
            Research Assistant, Sep. 2010 ~ Jun. 2012 <br>
            Supervisor: Prof. <a href="https://xxxy.fafu.edu.cn/90/65/c2743a233573/page.htm">Lijin Wang</a>, Prof. <a href="https://xxxy.fafu.edu.cn/cd/50/c2743a183632/page.htm">Yiwen Zhong</a> <br>
          </p>
       </li>
       <li>
          <p>
            Fujian Agriculture and Forestry University, China<br>
            Research Assistant, Sep. 2006 ~ Jun. 2010 <br>
            Supervisor: Prof. <a href="https://xxxy.fafu.edu.cn/cd/3c/c7843a183612/page.psp">Shiguo Huang</a> <br>
          </p>
       </li>-->
      </ul>
     
      <h2><a>Representative Publications</a> (<a href="https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=zh-CN&oi=ao">Full</a>)</h2>
      <ul>
        <li>
          <p>
            <b>Salient Objects in Clutter</b><br>
            <strong>Deng-Ping Fan</strong>, Jing Zhang, Gang Xu, Ming-Ming Cheng*, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023, 45 (2): 2344-2366. (IF: 24.314)<br>
            [Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_12" style="color:#2E41DC;"><u>ECCV 2018</u></a>]<br>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]SOC.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]SOC_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/SODBenchmark">[Project Page]</a>
            <a href="https://github.com/DengPingFan/SOC-DataAug">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9755062">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Salient Object Detection via Integrity Learning</b><br>
            Mingchen Zhuge^, <strong>Deng-Ping Fan^</strong>, Nian Liu*, Dingwen Zhang*, Dong Xu, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023. (IF: 24.314)<br>
            <a href="https://arxiv.org/abs/2101.07663">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]ICON_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/mczhuge/ICON">[Code]</a>
            <a class="github-button" href="https://github.com/mczhuge/ICON" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9789317">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Concealed Object Detection</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Ming-Ming Cheng*, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (10): 6024-6042. (IF: 24.314)<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a> | <strong><font color="red"><a href="https://dengpingfan.github.io/papers/SINet-V2-Award.pdf"><u>JDC 2021</u></a> Distinguish Paper</font></strong>]<br>
            <a href="https://arxiv.org/abs/2102.10274">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][PAMI]SINetV2_Chinese.pdf">[中译版]</a>
            <a href="https://dengpingfan.github.io/pages/COD.html">[Project Page]</a>
            <a href="http://mmcheng.net/cod/">[Online Demo]</a> 
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]ConcealedOD_supp.pdf">[Supplementary Material]</a>
            <a href="https://github.com/GewelsJI/SINet-V2">[Code-Python]</a>
            <a class="github-button" href="https://github.com/GewelsJI/SINet-V2" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://cg.cs.tsinghua.edu.cn/jittor/news/2021-06-11-00-00-cod/">[Code-Jittor]</a>
            <a href="https://pan.baidu.com/s/1EtH2tUdbBt16w5dgve7JhQ">[2.25G COD10K_All (Baidu: w3up)]</a>
            <a href="https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view?usp=sharing">[2.25G COD10K_All (Google)]</a>
            <a href="">[微信群 (WeChat: CVer222)]</a>
            <a href="https://ieeexplore.ieee.org/document/9444794">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Re-thinking co-salient object detection</b><br>
            <strong>Deng-Ping Fan</strong>, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng*, Huazhu Fu, Jianbing Shen<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (8): 4339-4354. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2007.03380v4">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TPAMI]CoSOD3k_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/CoEGNet?utm_source=catalyzex.com">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9358006">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Siamese Network for RGB-D Salient Object Detection and Beyond</b><br>
            Keren Fu, <strong>Deng-Ping Fan*</strong>, Ge-Peng Ji, Qijun Zhao, Jianbing Shen, Ce Zhu<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (9): 5541-5559. (IF: 24.314)<br>
            [<strong><font color="red">ESI Hot Paper (0.1%)</font></strong> | Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u>]</a><br>
            <a href="https://arxiv.org/abs/2008.12134">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TPAMI]JLDCF_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/JLDCF">[Code]</a>
            <a class="github-button" href="https://github.com/kerenfu/JLDCF" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9406382">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Uncertainty Inspired RGB-D Saliency Detection</b><br>
            Jing Zhang, <strong>Deng-Ping Fan*</strong>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Sadegh Aliakbarian, Nick Barnes<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (9): 5761-5779. (IF: 24.314)<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2009.03075">[PDF]</a>
            <a href="https://github.com/JingZhang617/UCNet?utm_source=catalyzex.com">[Code]</a>
            <a class="github-button" href="https://github.com/JingZhang617/UCNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9405467">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</b><br>
            Wenhai Wang, Enze Xie, Xiang Li, <strong>Deng-Ping Fan*</strong>, Kaitao Song, Ding Liang, Tong Lu*, Ping Luo, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Vitural, October 11-17, 2021<br>
            A pure Transformer backbone for dense prediction, such as object detection and semantic segmentation.<br>
            [<strong><font color="red">Most Influential ICCV2021 Papers 
            (<u><a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/" style="color:#2E41DC;">Top-2</a></u>)
            </strong></font>, Rate = 10/1617 = 0.62% | <strong><font color="red">Oral</strong></font>, Accept rate = 210/6236 = 3.4%]<br>
            <a href="https://arxiv.org/abs/2102.12122">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]PVT_Chinese.pdf">[中译版]</a>
            <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]PVT-ppt.pdf">[PPT]</a>
            <a href="https://github.com/whai362/PVT">[Code]</a>
            <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>UC-Net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders*</b><br>
            Jing Zhang, <strong>Deng-Ping Fan*</strong>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, Nick Barnes<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            [<strong><font color="red">Best Paper Nomination & Oral</font></strong>]<br>
            <a href="https://arxiv.org/abs/2004.05763">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]UCNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/JingZhang617/UCNet">[Code]</a>
            <a class="github-button" href="https://github.com/JingZhang617/UCNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Camouflaged object detection</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            [<strong><font color="red">Oral</font></strong>, Accept rate = 335/5865 = 5.7% | <strong><font color="red">Coverage of 《<a href="https://dengpingfan.github.io/papers/《New Scientist》Coverage.pdf">New scientist</a>》</font></strong>]<br>
            [PDF]
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]COD_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/SINet/">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/SINet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://dengpingfan.github.io/pages/COD.html">[Project Page]</a>
            <a href="http://mmcheng.net/cod/">[Online Demo]</a> 
            <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_60a36389e4b0adb2d8652c35/3">[1小时解读]</a>
            <a href="https://mmcheng.net/wp-content/uploads/2022/02/1052-oral.mp4">[Video]</a>
            <a href="https://pan.baidu.com/s/1EtH2tUdbBt16w5dgve7JhQ">[2.25G COD10K_All (Baidu: w3up)]</a>
            <a href="https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view?usp=sharing">[2.25G COD10K_All (Google)]</a>
            <a href="">[微信群 (WeChat: CVer222)]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
           <li>
          <p>
            <b>Inf-Net: Automatic Covid-19 Lung Infection Segmentation from CT Images</b><br>
            <strong>Deng-Ping Fan</strong>, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen*, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Transactions on Medical Imaging (TMI)</i>, 2020, 39 (8): 2626-2637. (IF: 11.037)<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | 
            <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=42" style="color:#2E41DC;"> <u>TMI 50 most frequently accessed articles</u></a> | 
            <a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=wqLkMlos2DIJ.2022" style="color:#2E41DC;"> <u>Top-20 Most Cited Paper within 5 Years in IEEE TMI2020</u></a> |
            Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26" style="color:#2E41DC;"><u>MICCAI 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2004.14133">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][TMI]InfNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/Inf-Net?utm_source=catalyzex.com">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/Inf-Net" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9098956">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>PraNet: Parallel reverse attention network for polyp segmentation</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>Medical Image Computing and Computer Assisted Intervention (MICCAI)</i>, Vitural, October 4-8, 2020<br>
            [<strong><font color="red">Early accept & Oral</font></strong>, Accept rate = 13% | 
            <strong><a href="https://dengpingfan.github.io/papers/PraNet-Award.pdf"><u>JDC 2021</u></a><font color="red"> Most Influential (Application) Paper</font></strong> | 
            <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_hcore&venue=QLpioUFGyGMJ.2022" style="color:#2E41DC;"> <u>Top-10 Most Cited Paper within 5 Years in MICCAI 2020</u></a> |
            <strong><font color="red">Best precision</font> of MediaEval 2020 Workshop at Medico Track</strong>]<br>
            <a href="https://arxiv.org/abs/2006.11392">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][MICCAI]PraNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/PraNet">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/PraNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Shifting More Attention to Video Salient Object Detection</b><br>
            <strong>Deng-Ping Fan</strong>, Wenguan Wang, Ming-Ming Cheng*, Jianbing Shen<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Long Beach, USA, June 16-20, 2019<br>
            [<strong><font color="red">Best Paper Finalist & Oral</font></strong>]<br>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2019][CVPR]SSAV_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/DAVSOD">[Project Page]</a>
            <a href="https://github.com/DengPingFan/DAVSOD">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/DAVSOD" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Structure-measure: A New Way to Evaluate Foreground Maps</b><br>
            <strong>Deng-Ping Fan</strong>, Ming-Ming Cheng*, Yun Liu, Tao Li, Ali Borji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Venice, Italy, October 22-29, 2017<br>
            [<strong><font color="red">Spotlight</font></strong>, Accept rate = 56/2143 = 2.6%]<br>
            <a href="https://arxiv.org/abs/1708.00786">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2017][ICCV]S-measure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://drive.google.com/file/d/10c_PU4pa-7KJ-c2puLfTRE0ZBWgPBwiS/view?usp=sharing">[Meta-measure]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2017_S-measure_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html">[Official Version]</a>
          </p>
        </li> 
      </ul>

      <!-- Teams -->
      <h2> 
        <a id="Teams" class = "anchor" href="#Teams" >Teams</a>
      </h2>
      <ul> 
        <li>
          <p>
            <b>Ph.D. Students</b><br>
            <!--<a href="https://scholar.google.com/citations?user=aCKl1R0AAAAJ&hl=zh-CN"><u>Zheng Lin</u></a>, Nankai University, Sep. 2019- (CVPR2020, TPAMI2021, Co-advise with Prof. Ming-Ming Cheng)<br>-->
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=LuqbqmAAAAAJ"><u>Zhou Huang</u></a>, UESTC & MBZUAI, Sep. 2021-Sep. 2022 (CSC Funding, Co-advise with Prof. Huai-Xin Chen)<br>
            <a href="https://scholar.google.com/citations?user=xmWZWwoAAAAJ&hl=zh-CN"><u>Shuo Wang</u></a>, Beijing Normal University & ETH Zurich, Dec. 2021- Now (CVPR2023-Submission, CSC Funding, Co-advise with Prof. Jiang Zhang)<br>
            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=bYLzUgYAAAAJ"><u>Xinyu Yan</u></a>, Tianjin University & MBZUAI, Dec. 2022- Now (CSC Funding, Co-advise with Prof. Zheng Wang)<br>
          </p>
        </li>
        <li>
          <p>
            <b>Master Students</b><br>
            <a href="https://github.com/zyjwuyan"><u>Yingjie Zhai</u></a>, Nankai University, Sep. 2019- Sep. 2021 (ECCV2020, TIP2021, Co-advise with Prof. Jufeng Yang)<br>
            <a href="https://www.linkedin.com/in/tinglei-feng-b52666235/"><u>Tinglei Feng</u></a>, Nankai University, Sep. 2021- Now (TPAMI2023, Co-advise with Prof. Jufeng Yang)<br>
            <a href="https://github.com/PanchengZhao"><u>Pancheng Zhao</u></a>, Nankai University, Dec. 2022- Now (Co-advise with Prof. Jufeng Yang)<br>
            <a href="https://1170801121.github.io/"><u>Zhaochong An</u></a>, ETH Zurich, Jan. 2022- Jan. 2023 (CVPR2023-Submission, Semester Project, Co-advise with Prof. Luc Van Gool)<br>   
            <a href="https://www.linkedin.com/in/xuejing-luo-632a24204?originalSubdomain=cn"><u>Xuejing Luo</u></a>, ETH Zurich, Sep. 2022- Now (Semester Project, Co-advise with Prof. Luc Van Gool)<br>  
            <a href="https://github.com/RascalGdd"><u>Diandian Guo</u></a>, University of Stuttgart, March 2023- Sep. 2023 (Master Thesis, Co-advise with Prof. Luc Van Gool)<br>
          </p>
        </li>
        <li>
          <p>
            <b>Undergraduate Students</b><br>
            <a href="https://scholar.google.com/citations?user=YVNRBTcAAAAJ&hl=zh-CN"><u>Yu-Cheng Chou</u></a>, B.S.@WHU (Now Ph.D.@Johns Hopkins University), Jan. 2021-Jun. 2022 (MICCAI2021, CCF-YEF-2022优秀大学生学术秀本科组一等奖: 1 in China)<br>
          </p>
        </li>
        <li>
          <p>
            <b>Research Intern (IIAI)/Visiting Scholar (MBZUAI)</b><br>
            <a href="https://scholar.google.com/citations?user=Qa1DMv8AAAAJ&hl=zh-CN"><u>Jing Zhang</u></a>, Ph.D.@ANU (Now Lecturer@ANU), November 2020-Sep. 2021 (CVPR2020-Best Paper Nomination, ICCV2021, TPAMI2022)</br>
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=1lPivLsAAAAJ"><u>Jialun Pei</u></a>, Ph.D.@HUST (Now Postdoctoral@CUHK), March 2021-March 2022 (ECCV2022)<br>
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=3lMuodUAAAAJ"><u>Xiaobin Hu</u></a>, Ph.D.@TUM (Now, Tencent Youtu Lab), July 2021-Dec. 2021 (ECCV2022)</br>
            <a href="https://scholar.google.com/citations?user=gQ5kHH8AAAAJ&hl=en"><u>Xuelian Cheng</u></a>, Ph.D.@Monash University, July 2021-Oct. 2021 (CVPR2022)<br>
            <a href="https://scholar.google.com/citations?user=Qnj6XlMAAAAJ&hl=zh-CN"><u>Mingchen Zhuge</u></a>, Master@CUG (Now Ph.D.@KAUST), Feb. 2020-Oct. 2022 (CVPR2021, TPAMI2023)</br>
            <a href="https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=zh-CN"><u>Ge-Peng Ji</u></a>, Master@WHU (Now Ph.D.@ANU), Feb. 2019-Jun 2021 (MICCAI2021-Student Travel Award, ICCV2021)<br>
            <a href="https://github.com/ZhengPeng7"><u>Peng Zheng</u></a>, Master@UNITN&Aalto University, April 2021-Now (TPAMI2023-Minor)</br>

          </p>
        </li>
        <li>
          <p>
            <b>IIAI-CV&Med Team Members</b><br>
            <a href="https://scholar.google.com/citations?user=MMLkNtYAAAAJ&hl=zh-CN"><u>Xuebin Qin</u></a>, Postdoctoral (Huawei; MBZUAI)<br>
            <a href="https://scholar.google.com/citations?user=TK-hRO8AAAAJ&hl=en"><u>Xin Li</u></a>, Senior Data Scientist (G42@AIQ; IIAI)<br>
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=FSfSgwQAAAAJ"><u>Fan Yang</u></a>, Senior Data Scientist (G42@AIQ; IIAI)<br>
            <a href="https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=zh-CN"><u>Prof. Geng Chen</u></a>, Research Scientist (NPU; IIAI)<br>
            <a href="https://scholar.google.com/citations?user=LPPsgWUAAAAJ&hl=zh-CN"><u>Prof. Tao Zhou</u></a>, Research Scientist (NJUST; IIAI)<br>
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=5uQEWX0AAAAJ"><u>Tianzhu Xiang</u></a>, Technique Lead (G42@Bayanat; IIAI)<br>
          </p>
        </li>
      </ul>
     
      <!-- Awards and Honors -->
      <h2> 
        <a id="Awards" class = "anchor" href="#Awards" >Awards and Honors</a>
      </h2>
      <ul> 
        <li>
          <p>
            <strong><a>Machine Intelligence Research 2022-Outstanding Associate Editor</a></strong></a>, <a href="https://mp.weixin.qq.com/s/lcWLKSzUDGz0xBDpYchUCQ">link</a> (5/46), 2023<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="https://elsevier.digitalcommonsdata.com/public-files/datasets/btchxktzyw/files/458f56f3-72b3-4a3b-a179-93d9c931a51f/file_downloaded">Top 2% of Scientists on Stanford List</a></strong>, <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/4">link</a>, 2022<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="https://elsevier.digitalcommonsdata.com/public-files/datasets/btchxktzyw/files/b3e31af2-054c-4b3a-b9c5-6fd9bf10557a/file_downloaded">Top 2% of Scientists on Stanford List</a></strong>, <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/3">link</a>, 2021<br>
          </p>
        </li>  
        <li>
          <p>
            <strong><a href="https://mp.weixin.qq.com/s/G0bsizT8ZPa0oXuSlXRTPQ">China Computer Federation (CCF) Outstanding Doctoral Dissertation Award</a></strong>, 2021 (10 in China)<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="https://dengpingfan.github.io/papers/SINet-V2-Award.pdf">Distinguish Paper</a></strong> in Jittor Developer Conference, 2021<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="https://dengpingfan.github.io/papers/PraNet-Award.pdf">Most Influential (Application) Paper</a></strong> in Jittor Developer Conference, 2021 (1 in JDC2021)<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="http://www.caai.cn/index.php?s=/home/article/detail/id/1162.html">The Prize of Second Class for the Wu Wenjun Natural Science Award (吴文俊人工智能自然科学二等奖)</a> (Rank 4)</strong>, 2020<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="http://cvpr2020.thecvf.com/node/817#best-paper-award">CVPR Best Paper Nomination</a></strong>, 2020<br>
          </p>
        </li>
        <li>
          <p>
            <strong><a href="https://cvpr2019.thecvf.com/files/CVPR%202019%20-%20Welcome%20Slides%20Final.pdf">CVPR Best Paper Finalist</a></strong>, 2019<br>
          </p>
        </li>
        <li>
          <p>
            Outstanding/Distinguished/Honorable Mention/High-Quality Reviewer: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8954223">CVPR 2019 with special mention</a> (25/2887), 
            <!--<a href="http://cvpr2020.thecvf.com/reviewer-acknowledgements#outstanding-reviewers">CVPR 2020</a> (141/3664),--> 
            <a href="http://cvpr2021.thecvf.com/node/184">CVPR 2021</a> (1065/5246), 
            <a href="https://dengpingfan.github.io/images/eccv2020-reviewer-certificate.pdf">ECCV 2020</a>,
            <a href="https://dengpingfan.github.io/images/tmi2022-reviewer-certificate.pdf">IEEE TMI Distinguished Reviewer 2020 – 2022</a>,
            <a href="https://conferences.miccai.org/2022/en/OUTSTANDING-REVIEWER-AWARDS.html">MICCAI 2022</a> (102/1242)
            <br>
          </p>
        </li>
        <li>
          <p>
            Outstanding Ph.D. Graduates in Nankai University, 2019 (1 in Colleage of Computer Science)
            <br>
          </p>
        </li>
        <li>
          <p>
            Huawei Ph.D. Fellowship, 2017 (1 in Colleage of Software)
            <br>
          </p>
        </li>
        <li>
          <p>
            Tianjin Wuqing District International Marathon-Men's Full Marathon (42.195Km) Personal Best (天津市武清区国际马拉松-男子全程马拉松个人最佳), 3 hours 52 minutes 01 second, 2016
            <br>
          </p>
        </li>
        <li>
          <p>
            Top-10 Running Stars (校十大跑星), Nankai University, 2016
            <br>
          </p>
        </li>
      </ul>

      <!-- Teaching -->
      <h2> 
        <a id="Teaching" class = "anchor" href="#Teaching" >Teaching Assistant</a>
      </h2>
      <ul> 
        <li>
          <p>
            2015.10 – 2016.1, Nankai University, BO Ren, <b>Linear Algebra</b>.
            <br>
          </p>
        </li>
        <li>
          <p>
             2016.3 – 2016.7, Nankai University, Ming-Ming Cheng, <b>Computer Vision</b>.
          </p>
        </li>
      </ul>

      <div id="footer">
        <div id="footer-text"></div>
      </div>
      © Deng-Ping Fan

      <div class="container">
         <div style="display:inline-block;width:200px;">
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5d4snb3frum&amp;s=220&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
        </div>
      </div>

    </div>
  </body>
</html>
