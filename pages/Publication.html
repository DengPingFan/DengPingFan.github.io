<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Deng-Ping Fan&#39;s home page">
    <link rel="shortcut icon" href="https://dengpingfan.github.io/images/logo-ethz2.png">
    <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://dengpingfan.github.io/assets/jemdoc.css" type="text/css">
    
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88572407-1', 'auto');
      ga('send', 'pageview');
    </script>
    <meta name="google-site-verification" content="F0Q0t5oLq1pGwXGMf_38oA2MxW_zfiMRsQTYD4_GJoQ"/>
    <title>Deng-Ping Fan</title>
  </head>

  <body>
    
    <div id="layout-content" style="margin-top:25px">
      <table>
        <tbody>
          <div id="toptitle">
            <h1>Publications</h1>
          </div>

          <div class="navbar-collapse collapse">
            <h2>
              <a href="https://dengpingfan.github.io/index.html">Home|</a>
              <a href="People.html">People|</a>
              <a href="Publication.html">Publications|</a>
              <a href="Services.html">Services|</a>  
              <a href="Accept.html">AcceptRate</a>   
            </h2>
          </div>    
          <p>
            ^ Equal Contribution, * Corresponding Author(s)
          </p>
        </tbody>
      </table>
      
      <!-- Book -->
      <h2> 
        <a id="Book" class="anchor" href="#Book">Book & Chapters</a> 
      </h2>
      <ul>
       <li>
         <table>
           <tbody>
            <tr>
              <td width="670">
                <img src="https://dengpingfan.github.io/images/SOD-Thesis.png" border="0" width="150">
              </td>
              <td>
                <u>范登平</u>. 认知规律启发的显著性物体检测方法与评测. 北京：机械工业出版社，2023 [PDF].<br>
              </td>
            </tr>
           </tbody>
         </table>
       </li>
       <li>
         Tao Zhou, <u>Deng-Ping Fan</u>, Gepeng Ji, Geng Chen, Huazhu Fu, Ling Shao, 2022. Inf-Net: an automatic lung infection segmentation network that uses CT images,
         in: Ayman El-Baz, Jasjit S Suri, Artificial Intelligence Strategies for Analyzing COVID-19 Pneumonia Lung Imaging, University of Louisville, Louisville, KY, USA, pp: 3-1 to 3-15.<br>
       </li>
      </ul>
      
      <!-- Preprints -->
      <h2> 
        <a id="Publications" class = "anchor" href="#Publications">Preprints</a>
      </h2>
      <ul>
        <li>
          <p>
            <b>Practical Blind Denoising via Swin-Conv-Unet and Data Synthesis</b><br>
            Kai Zhang*, Yawei Li, Jingyun Liang, Jiezhang Cao, Yulun Zhang, Hao Tang, <u>Deng-Ping Fan</u>, Radu Timofte, Luc Van Gool<br>         
            <i>Machine Intelligence Research (MIR)</i>-Submission, 2023. (中国国际影响力优秀学术期刊)<br>
            [PDF]
            [Code]
            [Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>Boundary-aware segmentation network for mobile and web applications</b><br>
            Xuebin Qin, <u>Deng-Ping Fan*</u>, Chenyang Huang, Cyril Diagne, Zichen Zhang, Adrià Cabeza Sant'Anna, Albert Suarez, Martin Jagersand, Ling Shao<br>
            <i>Technique Report</i>, 2022<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html" style="color:#2E41DC;"><u>CVPR 2019</u></a>]<br>
            <a href="https://arxiv.org/abs/2101.04704">[PDF]</a>
            <a href="https://cloud.tencent.com/developer/article/1688893">[中文解读]</a>
            <a href="https://github.com/xuebinqin/BASNet">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/xuebinqin/BASNet?style=social"/>-->
            <a class="github-button" href="https://github.com/xuebinqin/BASNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://www.youtube.com/watch?v=VxJmS8avjbY">[Tutorial (4k views)]</a>
            <a href="https://weibo.com/tv/show/1034:4500977827381257?from=old_pc_videoshow">[Webo (1,070k views)]</a>        
            <a class="github-button" href="https://github.com/cyrildiagne/ar-cutpaste" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://dengpingfan.github.io/videos/SOD-App-BASNetAR.mp4">[Video]</a>
          </p>
        </li>
        <!--<li>
          <p>
            <b>Distraction-Aware Camouflaged Object Segmentation</b><br>
            Haiyang Mei, Xin Yang*, Xiaopeng Wei, Ge-Peng Ji, <u>Deng-Ping Fan</u><br>
            <i>arXiv</i>, 2022<br>
          </p>
        </li>-->
      </ul>

      <h2><a>Camouflaged Scene Understanding</a></h2>
      <ul>
        <br><a><strong>COD</strong></a><br>
        <li>
          <p>
            <b>Masked Separable Attention for Camouflaged Object Detection</b><br>
            Bowen Yin, Xuying Zhang, Qibin Hou*, Bo-Yuan Sun, <u>Deng-Ping Fan</u>, Luc Van Gool<br>
            <i>Technique Report</i>, 2023<br>
            <a href="https://arxiv.org/abs/2212.06570">[PDF]</a>
            [中译版]
            <a href="https://github.com/HVision-NKU/CamoFormer">[Code]</a>
            [Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>分心感知的伪装物体分割</b><br>
            梅海洋， 杨鑫*， 周运铎， 季葛鹏， 魏小鹏， <u>范登平</u><br>
            <i>中国科学：信息科学</i>, 2023, 大修. (IF: 7.275, CCF A类, CCF T1)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Camouflaged_Object_Segmentation_With_Distraction_Mining_CVPR_2021_paper.pdf" style="color:#2E41DC;"><u>CVPR 2021</u></a>]<br>
            [PDF][Code][Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>Deep Gradient Learning for Efficient Camouflaged Object Detection</b><br>
            Ge-Peng Ji, <u>Deng-Ping Fan*</u>, Yu-Cheng Chou, Dengxin Dai, Alexander Liniger, Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2023. (中国国际影响力优秀学术期刊)<br>
            <a href="https://arxiv.org/pdf/2205.12853v2.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][MIR]DGNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/DGNet">[Code]</a>
            <a class="github-button" href="https://github.com/GewelsJI/DGNet" data-icon="octicon-star" data-show-count="true">Star</a>
          </p>
        </li>
        <li>
            <p>
              <b>Towards Deeper Understanding of Camouflaged Object Detection</b><br>
              Yunqiu Lv^, Jing Zhang^, Yuchao Dai*, Aixuan Li, Nick Barnes, <u>Deng-Ping Fan</u><br>
              <i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), Major</i>, 2023. (IF: 5.859)<br>
              [Extension of <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Simultaneously_Localize_Segment_and_Rank_the_Camouflaged_Objects_CVPR_2021_paper.pdf" style="color:#2E41DC;"><u>CVPR 2021</u></a>]<br>
              <a href="https://arxiv.org/pdf/2205.11333">[PDF]</a>
              <a href="https://github.com/JingZhang617/COD-Rank-Localize-and-Segment">[Code]</a>
            </p>
        </li>
        <li>
          <p>
            <b>Concealed Object Detection</b><br>
            <u>Deng-Ping Fan</u>, Ge-Peng Ji, Ming-Ming Cheng*, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (10): 6024-6042. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a> | <strong><font color="red"><a href="https://dengpingfan.github.io/papers/SINet-V2-Award.pdf"><u>JDC 2021</u></a> Distinguish Paper</font></strong>]<br>
            <a href="https://arxiv.org/abs/2102.10274">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][PAMI]SINetV2_Chinese.pdf">[中译版]</a>
            <a href="https://dengpingfan.github.io/pages/COD.html">[Project Page]</a>
            <a href="http://mmcheng.net/cod/">[Online Demo]</a> 
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]ConcealedOD_supp.pdf">[Supplementary Material]</a>
            <a href="https://github.com/GewelsJI/SINet-V2">[Code-Python]</a>        
            <a class="github-button" href="https://github.com/GewelsJI/SINet-V2" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://cg.cs.tsinghua.edu.cn/jittor/news/2021-06-11-00-00-cod/">[Code-Jittor]</a>
            <a href="https://pan.baidu.com/s/1EtH2tUdbBt16w5dgve7JhQ">[2.25G COD10K_All (Baidu: w3up)]</a>
            <a href="https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view?usp=sharing">[2.25G COD10K_All (Google)]</a>
            <a href="">[微信群 (WeChat: CVer222)]</a>
            <a href="https://ieeexplore.ieee.org/document/9444794">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers</b><br>
            Jialun Pei^, Tianyang Cheng^, <u>Deng-Ping Fan*</u>, He Tang, Chuanbo Chen, Luc Van Gool<br>
            <i>European Conference on Computer Vision (ECCV)</i>, Tel Aviv, Israel, October 23-27, 2022<br>
            <a href="https://dengpingfan.github.io/papers/[2022][ECCV]OSFormer.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][ECCV]OSFormer_Chinese.pdf">[中译版]</a>
            <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_631055f0e4b0eca59c2ab21d">[视频解读]</a>
            <a href="https://blog.patrickcty.cc/OSFormer-Homepage/">[Project Page]</a>
            <a href="https://github.com/PJLallen/OSFormer">[Code]</a>
            <a class="github-button" href="https://github.com/PJLallen/OSFormer" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-19797-0_2">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Mutual Graph Learning for Camouflaged Object Detection</b><br>
            Qiang Zhai^, Xin Li^, Fan Yang*, Chenglizhao Chen, Hong Cheng, <u>Deng-Ping Fan</u><br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Vitural, June 19-25, 2021<br>
            <a href="https://arxiv.org/abs/2104.02613">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][CVPR]MGL_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/fanyang587/MGL">[Code]</a>
            <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zhai_Mutual_Graph_Learning_for_Camouflaged_Object_Detection_CVPR_2021_paper.pdf">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Camouflaged Object Segmentation with Distraction Mining</b><br>
            Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang*, Xiaopeng Wei, <u>Deng-Ping Fan</u><br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Vitural, June 19-25, 2021<br>
            <a href="https://arxiv.org/abs/2104.10475">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][CVPR]PFNet_Chinese.pdf">[中译版]</a>
            <a href="https://mhaiyang.github.io/CVPR2021_PFNet/index">[Project Page]</a>
            <a href="https://github.com/Mhaiyang/CVPR2021_PFNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Mei_Camouflaged_Object_Segmentation_With_Distraction_Mining_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Camouflaged object detection</b><br>
            <u>Deng-Ping Fan</u>, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            [<strong><font color="red">Oral</font></strong>, Accept rate = 335/5865 = 5.7% | <strong><font color="red">Coverage of 《<a href="papers/《New Scientist》Coverage.pdf">New scientist</a>》</font></strong>]<br>
            [PDF]
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]COD_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/SINet/">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/SINet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://dengpingfan.github.io/pages/COD.html">[Project Page]</a>
            <a href="http://mmcheng.net/cod/">[Online Demo]</a> 
            <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_60a36389e4b0adb2d8652c35/3">[1小时解读]</a>
            <a href="https://mmcheng.net/wp-content/uploads/2022/02/1052-oral.mp4">[Video]</a>
            <a href="https://pan.baidu.com/s/1EtH2tUdbBt16w5dgve7JhQ">[2.25G COD10K_All (Baidu: w3up)]</a>
            <a href="https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view?usp=sharing">[2.25G COD10K_All (Google)]</a>
            <a href="">[微信群 (WeChat: CVer222)]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>Video COD</strong></a><br>
        <li>
          <p>
            <b>Implicit Motion Handling for Video Camouflaged Object Detection</b><br>
            Xuelian Cheng^, Huan Xiong^, <u>Deng-Ping Fan*</u>, Yiran Zhong, Mehrtash Harandi, Tom Drummond, Zongyuan Ge<br> 
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, New Orleans, USA, June 19-24, 2022<br>
            <a href="https://arxiv.org/abs/2203.07363">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][CVPR]VCOD_MoCA-Mask_Chinese.pdf">[中译版]</a>
            <a href="https://youtu.be/jG01QUk4wjk">[Video]</a>
            <a href="https://xueliancheng.github.io/SLT-Net-project/">[Project Page]</a>
            <a href="https://github.com/XuelianCheng/SLT-Net">[Code]</a>
            <a class="github-button" href="https://github.com/XuelianCheng/SLT-Net" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Implicit_Motion_Handling_for_Video_Camouflaged_Object_Detection_CVPR_2022_paper.html">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>RGB-D COD</strong></a><br>
        <li>
          <p>
            <b>Source-free Depth for Object Pop-out</b><br>
            Zongwei Wu, Danda Pani Paudel, <u>Deng-Ping Fan*</u>, Jingjing Wang, Shuo Wang, Cédric Demonceaux, Radu Timofte, Luc Van Gool<br>
            <i>Technique Report</i>, 2023<br>
            <a href="https://arxiv.org/pdf/2212.05370">[PDF]</a>
            [中译版]
            [Code]
            [Official Version]
          </p>
        </li>
      </ul>
      
      <h2><a>AI in Healthcare</a></h2>
      <ul>
        <br><a><strong>Medical Image Segmentation</strong></a><br>
        <li>
          <p>
            <b>MAST: Video Polyp Segmentation with a Mixture-Attention Siamese Transformer</b><br>
            <i>Technique Report</i>, 2023<br>
            [PDF]
            [中译版]
            [Code]
            [Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>Video Polyp Segmentation: A Deep Learning Perspective</b><br>
            Ge-Peng Ji^, Guobao Xiao^, Yu-Cheng Chou^, <u>Deng-Ping Fan*</u>, Kai Zhao, Geng Chen, and Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2022, 19 (6): 531-549. (中国国际影响力优秀学术期刊)<br>
            <a href="https://arxiv.org/pdf/2203.14291v3.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][MIR]VPS_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/VPS">[Code]</a>
            <a class="github-button" href="https://github.com/GewelsJI/VPS" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/article/10.1007/s11633-022-1371-y">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers</b><br>
            Bo Dong, Wenhai Wang, <u>Deng-Ping Fan*</u>, Jinpeng Li, Huazhu Fu, Ling Shao<br>
            <i>CAAI Artificial Intelligence Research (AIR), Submission</i>, 2023. (中国卓越期刊行动计划)<br>
            <a href="https://arxiv.org/abs/2108.06932">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][SSI-Submission]PolypPVT_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/Polyp-PVT">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/Polyp-PVT" data-icon="octicon-star" data-show-count="true">Star</a>
          </p>
        </li>
        <li>
          <p>
            <b>Progressively Normalized Self-Attention Network for Video Polyp Segmentation</b><br>
            Ge-Peng Ji^, Yu-Cheng Chou^, <u>Deng-Ping Fan*</u>, Geng Chen, Huazhu Fu, Debesh Jha, Ling Shao<br>
            <i>Medical Image Computing and Computer Assisted Intervention (MICCAI)</i>, Strasbourg, France, September 27-October 1, 2021<br>
            [<strong><font color="red">Early Accept</font></strong>, Accept rate = 13% | <strong><font color="red">Student Travel Award</font></strong>, Rate = 95/1630 = 5.8%]<br>
            <a href="https://arxiv.org/abs/2105.08468">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][MICCAI]PNSNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/PNS-Net">[Code]</a>
            <a class="github-button" href="https://github.com/GewelsJI/PNS-Net" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://miccai2021.org/openaccess/paperlinks/2021/09/01/378-Paper0320.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>PraNet: Parallel reverse attention network for polyp segmentation</b><br>
            <u>Deng-Ping Fan</u>, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>Medical Image Computing and Computer Assisted Intervention (MICCAI)</i>, Vitural, October 4-8, 2020<br>
            [<strong><font color="red">Early accept & Oral</font></strong>, Accept rate = 13% | 
            <strong><a href="https://dengpingfan.github.io/papers/PraNet-Award.pdf"><u>JDC 2021</u></a><font color="red"> Most Influential (Application) Paper</font></strong> | 
            <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_hcore&venue=QLpioUFGyGMJ.2022" style="color:#2E41DC;"> <u>Top-10 Most Cited Paper within 5 Years in MICCAI 2020</u></a> |
            <strong><font color="red">Best precision</font> of MediaEval 2020 Workshop at Medico Track</strong>]<br>
            <a href="https://arxiv.org/abs/2006.11392">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][MICCAI]PraNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/PraNet">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/PraNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation</b><br>
            Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, <u>Deng-Ping Fan</u>, Rong-Guo Zhang, Ming-Ming Cheng*<br>
            <i>IEEE Transactions on Image Processing (TIP)</i>, 2021, 30: 3113-3126. (IF: 11.041)<br>
            [<strong><font color="red">ESI Hot Paper (0.1%)</font></strong>|第二十二届中国<strong><font color="red">国际工业博览会高校展区优秀展品特等奖</font></strong>]<br>
            <a href="https://arxiv.org/abs/2004.07054">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TIP]JCS_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/yuhuan-wu/JCS">[Code]</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9357961">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>COVID-19 Lung Infection Segmentation with A Novel Two-Stage Cross-Domain Transfer Learning Framework</b><br>
            Jiannan Liu, Bo Dong, Shuai Wang, Hui Cui, <u>Deng-Ping Fan</u>, Jiquan Ma*, Geng Chen*<br>
            <i>Medical Image Analysis (MIA)</i>, 2021, 74: 102205. (IF: 8.545)<br>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8342869/">[PDF]</a>
            <a href="https://github.com/Jiannan-Liu/nCoVSegNet">[Code]</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S1361841521002504?casa_token=4dIB3WQF8osAAAAA:WqK99Z08kXXrqrrX6DXK_6eWH_uX944wn_QaommMaJ45HItdj8Nc5EMS_DOUvjMTio476WmnLMI">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Inf-Net: Automatic Covid-19 Lung Infection Segmentation from CT Images</b><br>
            <u>Deng-Ping Fan</u>, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen*, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Transactions on Medical Imaging (TMI)</i>, 2020, 39 (8): 2626-2637. (IF: 11.037)<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | 
            <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=42" style="color:#2E41DC;"> <u>TMI 50 most frequently accessed articles</u></a> | 
            <a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=wqLkMlos2DIJ.2022" style="color:#2E41DC;"> <u>Top-20 Most Cited Paper within 5 Years in IEEE TMI2020</u></a> |
            Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26" style="color:#2E41DC;"><u>MICCAI 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2004.14133">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][TMI]InfNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/Inf-Net?utm_source=catalyzex.com">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/Inf-Net" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9098956">[Official Version]</a>
          </p>
        </li>
      </ul>
      
      <h2><a>Visual Attention</a></h2>
      <ul>
        <br><a><strong>RGB SOD</strong></a><br>
        <li>
          <p>
            <b>Salient Objects in Clutter</b><br>
            <u>Deng-Ping Fan</u>, Jing Zhang, Gang Xu, Ming-Ming Cheng*, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023. (IF: 24.314)<br>
            [Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_12" style="color:#2E41DC;"><u>ECCV 2018</u></a>]<br>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]SOC.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]SOC_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/SODBenchmark">[Project Page]</a>
            <a href="https://github.com/DengPingFan/SOC-DataAug">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9755062">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Salient Object Detection via Integrity Learning</b><br>
            Mingchen Zhuge^, <u>Deng-Ping Fan^</u>, Nian Liu*, Dingwen Zhang*, Dong Xu, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023. (IF: 24.314)<br>
            <a href="https://arxiv.org/abs/2101.07663">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI]ICON_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/mczhuge/ICON">[Code]</a>
            <a class="github-button" href="https://github.com/mczhuge/ICON" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9789317">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Generative Transformer for Accurate and Reliable Salient Object Detection</b><br>
            Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai*, Aixuan Li, Yunqiu Lv, Xinyu Tian, <u>Deng-Ping Fan</u>, Nick Barnes<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Major</i>, 2023. (IF: 24.314)<br>
            <a href="https://arxiv.org/abs/2104.10127">[PDF]</a>
            <a href="https://github.com/fupiao1998/TrasformerSOD">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>认知规律启发的显著性物体检测方法与评测</b><br>
            <b>Cognitive Inspirited Salient Object Detection Models and Benchmarks</b><br>
            <u>范登平</u><br>
            <u>Deng-Ping Fan</u><br>
            <i>南开大学博士学位论文</i>, 2019. (2021年<strong><a href="https://mp.weixin.qq.com/s/G0bsizT8ZPa0oXuSlXRTPQ">CCF优秀博士学位论文奖</a></strong>)<br>
            <i>Ph.D. thesis, Nankai University</i>, 2019 (<strong><a href="https://mp.weixin.qq.com/s/G0bsizT8ZPa0oXuSlXRTPQ">China Computer Federation (CCF) Outstanding Doctoral Dissertation Award</a></strong>)<br>
            <a href="https://dengpingfan.github.io/papers/[2019][Nankai University][PhD Thesis][Deng-Ping Fan]Cognitive Inspirited Salient Object Detection Models and Benchmarks-min.pdf">[PDF]</a>
            [PPT]
          </p>
        </li>
        <li>
          <p>
            <b>Salient objects in clutter: Bringing salient object detection to the foreground</b><br>
            <u>Deng-Ping Fan</u>, Ming-Ming Cheng*, Jiang-Jiang Liu, Shang-Hua Gao, Qibin Hou, Ali Borji<br>
            <i>European Conference on Computer Vision (ECCV)</i>, Munich, Germany, September 8-14, 2018<br>
            <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Deng-Ping_Fan_Salient_Objects_in_ECCV_2018_paper.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2018][ECCV]SOCBenchmark_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/socbenchmark/">[Project Page]</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_12">[Official Version]</a>
          </p>
        </li> 
        <br><a><strong>Video SOD</strong></a><br>
        <li>
          <p>
            <b>Weakly Supervised Visual-Auditory Fixation Prediction with Multigranularity Perception</b><br>
            Guotao Wang, Chenglizhao Chen*, <u>Deng-Ping Fan</u>, Aimin Hao, Hong Qin<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Major</i>, 2023. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_From_Semantic_Categories_to_Fixations_A_Novel_Weakly-Supervised_Visual-Auditory_Saliency_CVPR_2021_paper.html" style="color:#2E41DC;"><u>CVPR 2021</u></a>]<br>
            <a href="https://arxiv.org/abs/2112.13697">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][TPAMI-Major]SCAM_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/guotaowang/STANet">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Full-Duplex Strategy for Video Object Segmentation</b><br>
            Ge-Peng Ji, <u>Deng-Ping Fan*</u>, Keren Fu, Zhe Wu, Jianbing Shen, Ling Shao<br>
            <i>Computational Visual Media (CVMJ)</i>, 2023, 9 (1): 155–175 (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ji_Full-Duplex_Strategy_for_Video_Object_Segmentation_ICCV_2021_paper.html" style="color:#2E41DC;"><u>ICCV 2021</u></a>]<br>
            <a href="https://arxiv.org/abs/2108.03151">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]VSOD_FSNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/FSNet">[Code]</a>
            <a class="github-button" href="https://github.com/GewelsJI/FSNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/article/10.1007/s41095-021-0262-4">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Full-Duplex Strategy for Video Object Segmentation</b><br>
            Ge-Peng Ji, <u>Deng-Ping Fan*</u>, Keren Fu, Zhe Wu, Jianbing Shen, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Vitural, October 11-17, 2021<br>
            <a href="https://arxiv.org/abs/2108.03151">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]VSOD_FSNet_Chinese.pdf">[中译版]</a>        
            <a href="https://github.com/GewelsJI/FSNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ji_Full-Duplex_Strategy_for_Video_Object_Segmentation_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Shifting More Attention to Video Salient Object Detection</b><br>
            <u>Deng-Ping Fan</u>, Wenguan Wang, Ming-Ming Cheng*, Jianbing Shen<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Long Beach, USA, June 16-20, 2019<br>
            [<strong><font color="red">Best Paper Finalist & Oral</font></strong>]<br>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2019][CVPR]SSAV_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/DAVSOD">[Project Page]</a>
            <a href="https://github.com/DengPingFan/DAVSOD">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/DengPingFan/DAVSOD?style=social"/>-->
            <a class="github-button" href="https://github.com/DengPingFan/DAVSOD" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>Multi-modal SOD</strong></a><br>
        <li>
          <p>
            <b>GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector</b><br>
            Peng Zheng^, Huazhu Fu^, <u>Deng-Ping Fan*</u>, Qi Fan, Jie Qin, Yu-Wing Tai, Chi-Keung Tang, and Luc Van Gool<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Major</i>, 2023. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Group_Collaborative_Learning_for_Co-Salient_Object_Detection_CVPR_2021_paper.html" style="color:#2E41DC;"><u>CVPR 2021</u></a>]<br>
            <a href="https://arxiv.org/abs/2205.15469">[PDF]</a>
            [中译版]
            <a href="https://github.com/ZhengPeng7/GCoNet_plus">[Code]</a>
            [Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>Specificity-preserving RGB-D Saliency Detection</b><br>
            Tao Zhou, <u>Deng-Ping Fan*</u>, Geng Chen, Yi Zhou, Huazhu Fu<br>
            <i>Computational Visual Media (CVMJ)</i>, 2023. (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_Specificity-Preserving_RGB-D_Saliency_Detection_ICCV_2021_paper.html" style="color:#2E41DC;"><u>ICCV 2021</u></a>]<br>
            <a href="https://dengpingfan.github.io/papers/[2022][CVMJ]SPNet.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][CVMJ]SPNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/taozh2017/SPNet">[Code]</a>
            <a class="github-button" href="https://github.com/taozh2017/SPNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <!--<a href="">[Official Version]</a>-->
          </p>
        </li>
        <li>
          <p>
            <b>Light Field Salient Object Detection: A Review and Benchmark</b><br>
            Keren Fu, Yao Jiang, Ge-Peng Ji, Tao Zhou*, Qijun Zhao, <u>Deng-Ping Fan</u><br>
            <i>Computational Visual Media (CVMJ)</i>, 2022, 8 (4): 509–534. (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            <a href="https://arxiv.org/abs/2010.04968">[PDF]</a>
            <a href="http://www.kerenfu.top/sources/2021cvmlfsurvey-cn.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/LFSOD-Survey">[Code]</a>
            <a class="github-button" href="https://github.com/kerenfu/LFSOD-Survey" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/article/10.1007/s41095-021-0256-2">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Re-thinking co-salient object detection</b><br>
            <u>Deng-Ping Fan</u>, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng*, Huazhu Fu, Jianbing Shen<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (8): 4339-4354. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2007.03380v4">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TPAMI]CoSOD3k_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/CoEGNet?utm_source=catalyzex.com">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9358006">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Siamese Network for RGB-D Salient Object Detection and Beyond</b><br>
            Keren Fu, <u>Deng-Ping Fan*</u>, Ge-Peng Ji, Qijun Zhao, Jianbing Shen, Ce Zhu<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (9): 5541-5559. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u>]</a><br>
            <a href="https://arxiv.org/abs/2008.12134">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TPAMI]JLDCF_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/JLDCF">[Code]</a>
            <a class="github-button" href="https://github.com/kerenfu/JLDCF" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9406382">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Uncertainty Inspired RGB-D Saliency Detection</b><br>
            Jing Zhang, <u>Deng-Ping Fan*</u>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Sadegh Aliakbarian, Nick Barnes<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022, 44 (9): 5761-5779. (IF: 24.314)<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2009.03075">[PDF]</a>
            <a href="https://github.com/JingZhang617/UCNet?utm_source=catalyzex.com">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/JingZhang617/UCNet?style=social"/>-->
            <a class="github-button" href="https://github.com/JingZhang617/UCNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/document/9405467">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Group Collaborative Learning for Co-Salient Object Detection</b><br>
            Qi Fan, <u>Deng-Ping Fan*</u>, Huazhu Fu, Chi-Keung Tang, Ling Shao, Yu-Wing Tai<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Vitural, June 19-25, 2021<br>
            <a href="https://arxiv.org/abs/2104.01108">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][CVPR]GCoNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/fanq15/GCoNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Group_Collaborative_Learning_for_Co-Salient_Object_Detection_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Specificity-preserving RGB-D Saliency Detection</b><br>
            Tao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, <u>Deng-Ping Fan*</u>, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Vitural, October 11-17, 2021<br>
            <a href="https://arxiv.org/abs/2108.08162">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]SPNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/taozh2017/SPNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_Specificity-Preserving_RGB-D_Saliency_Detection_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>RGB-D saliency detection via cascaded mutual information minimization</b><br>
            Jing Zhang, <u>Deng-Ping Fan*</u>, Yuchao Dai, Xin Yu, Yiran Zhong, Nick Barnes, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Vitural, October 11-17, 2021<br>
            <a href="https://arxiv.org/abs/2109.07246">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]CMINet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/JingZhang617/cascaded_rgbd_sod">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_RGB-D_Saliency_Detection_via_Cascaded_Mutual_Information_Minimization_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</b><br>
            <u>Deng-Ping Fan</u>, Zheng Lin, Zhao Zhang, Menglong Zhu, Ming-Ming Cheng*<br>
            <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</i>, 2021, 32 (5): 2075-2089. (IF: 14.255)<br>
            [<strong><font color="red">ESI Hot Paper (0.1%)</font></strong> | <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_hcore&venue=LILlZnFGZh8J.2022&cstart=20" style="color:#2E41DC;"> <u>Top-5 Most Cited Paper in IEEE TNNLS2020</u></a>]<br>
            <a href="https://arxiv.org/abs/1907.06781">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][TNNLS]SIPBenchmark_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/D3NetBenchmark">[Project Page]</a>
            <a href="https://pan.baidu.com/s/1wMTDG8yhCNbioPwzq7t25w">[SIP1K (Baidu: 46w8)]</a>
            <a href="https://drive.google.com/file/d/1R91EEHzI1JwfqvQJLmyciAIWU-N8VR4A/view">[SIP1K (Google)]</a>
            <!--<img src="https://img.shields.io/github/stars/DengPingFan/D3NetBenchmark?style=social"/>-->
            <a class="github-button" href="https://github.com/DengPingFan/D3NetBenchmark" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9107477">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>RGB-D Salient Object Detection: A Survey</b><br>
            Tao Zhou, <u>Deng-Ping Fan*</u>, Ming-Ming Cheng, Jianbing Shen, Ling Shao<br>
            <i>Computational Visual Media (CVMJ)</i>, 2021, 7 (1): 37-69. (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | <a href="https://link.springer.com/content/pdf/10.1007/s41095-021-0261-5.pdf">Spotlight Paper</a> | <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_hcore&venue=w7EibwBrdVAJ.2022" style="color:#2E41DC;"> <u>Top-2 Most Cited Paper within 5 Years in CVMJ2021</u></a>]<br>
            <a href="https://arxiv.org/abs/2008.00230">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][CVMJ]RGBD_Survey_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/taozh2017/RGBD-SODsurvey">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/taozh2017/RGBD-SODsurvey?style=social"/>-->
            <a class="github-button" href="https://github.com/taozh2017/RGBD-SODsurvey" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/article/10.1007/s41095-020-0199-z">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Taking a deeper look at co-salient object detection</b><br>
            <u>Deng-Ping Fan^</u>, Zheng Lin^, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, Ming-Ming Cheng*<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            [PDF]
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]CoSOD3k_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/CoSOD3K">[Project Page]</a>
            <a href="https://dengpingfan.github.io/CoSalBenchmark-EvaluationTools.zip">[Evaluation Code]</a>
            <a href="https://pan.baidu.com/s/1FQZyg39it7pSoXrSa9k6VA">[CoSOD3K (Baidu: ky1v)]</a>
             <a href="https://drive.google.com/file/d/1nv20lZMMhHEOe9_elDHy2motgMtejCZM/view?usp=sharing">[CoSOD3K (Google)]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network</b><br>
            <u>Deng-Ping Fan^</u>, Yingjie Zhai^, Ali Borji, Jufeng Yang*, Ling Shao<br>
            <i>European Conference on Computer Vision (ECCV)</i>, Virtual, August 23-28, 2020<br>
            <a href="https://arxiv.org/abs/2007.02713v1">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][ECCV]BBSNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/zyjwuyan/BBS-Net">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/zyjwuyan/BBS-Net?style=social"/>-->
            <a class="github-button" href="https://github.com/zyjwuyan/BBS-Net" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-58610-2_17">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection</b><br>
            Keren Fu, <u>Deng-Ping Fan*</u>, Ge-Peng Ji, Qijun Zhao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            <a href="https://arxiv.org/abs/2004.08515">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]JLDCF_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/JLDCF/">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>UC-Net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders*</b><br>
            Jing Zhang, <u>Deng-Ping Fan*</u>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, Nick Barnes<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Virtual, June 14-19, 2020<br>
            [<strong><font color="red">Best Paper Nomination & Oral</font></strong>]<br>
            <a href="https://arxiv.org/abs/2004.05763">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2020][CVPR]UCNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/JingZhang617/UCNet">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/JingZhang617/UCNet?style=social"/>-->
            <a class="github-button" href="https://github.com/JingZhang617/UCNet" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection</b><br>
            Jia-Xing Zhao^, Yang Cao^, <u>Deng-Ping Fan^</u>, Ming-Ming Cheng*, Xuan-Yi Li, Le Zhang<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Long Beach, California, June 16-20, 2019<br>
            [PDF]
            <a href="https://dengpingfan.github.io/papers/[2019][CVPR]CPFP_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/rgbdsalpyr/">[Project Page]</a>
            <a href="https://github.com/JXingZhao/ContrastPrior">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Contrast_Prior_and_Fluid_Pyramid_Integration_for_RGBD_Salient_Object_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
      </ul>

      <h2><a>Miscellaneous</a></h2>
      <ul>
        <br><a><strong>Metrics</strong></a><br>
        <li>
          <p>
            <b>IC9600: A Benchmark Dataset for Automatic Image Complexity Assessment</b><br>
            Tinglei Feng^, Yingjie Zhai^, Jufeng Yang, Jie Liang, <u>Deng-Ping Fan</u>, Jing Zhang, Ling Shao, Dacheng Tao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023. (IF: 24.314)<br>
            [PDF][中译版][Code][Official Version]
          </p>
        </li>
        <li>
          <p>
            <b>Structure-measure: A new way to evaluate foreground maps</b><br>
            Ming-Ming Cheng*, <u>Deng-Ping Fan</u><br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2021, 129 (9): 2622-2638. (IF: 13.369)<br>
            [Extension of <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html" style="color:#2E41DC;"><u>ICCV 2017</u></a>]<br>
            <a href="https://dengpingfan.github.io/papers/[2022][IJCV]Structure-Measure A New WaytoEvaluateForegroundMaps.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][IJCV]Smeasure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://drive.google.com/file/d/10c_PU4pa-7KJ-c2puLfTRE0ZBWgPBwiS/view?usp=sharing">[Meta-measure]</a>
            <a href="https://link.springer.com/article/10.1007/s11263-021-01490-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>认知规律启发的物体分割评价标准及损失函数</b><br>
            <u>范登平</u>， 季葛鹏， 秦雪彬， 程明明*<br>
            <i>中国科学：信息科学</i>, 2021, 51 (9): 1475-1489. (IF: 7.275, CCF A类, CCF T1)<br>
            [Extension of <a href="https://www.ijcai.org/proceedings/2018/97" style="color:#2E41DC;"><u>IJCAI 2018</u></a>]<br>
            <a href="https://dengpingfan.github.io/papers/[2021][SSI]EmeasureEng.pdf">[PDF]</a>
            <a href="https://github.com/GewelsJI/Hybrid-Eloss/">[Code]</a>
            <a href="https://www.sciengine.com/publisher/scp/journal/SSI/51/9/10.1360/SSI-2020-0370?slug=fulltext">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Scoot: A Perceptual Metric for Facial Sketches</b><br>
            <u>Deng-Ping Fan</u>, ShengChuan Zhang, Yu-Huan Wu, Yun Liu, Ming-Ming Cheng*, Bo Ren, Paul Rosin, Rongrong Ji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Seoul, Korea, October 27-November 02, 2019<br>
            <a href="https://arxiv.org/abs/1908.08433.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2019][ICCV]Scoot_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/scoot/">[Project Page]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2019_Scoot_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/DengPingFan/Scoot">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Enhanced-alignment measure for binary foreground map evaluation</b><br>
            <u>Deng-Ping Fan</u>, Cheng Gong, Yang Cao, Bo Ren*, Ming-Ming Cheng, Ali Borji<br>
            <i>International Joint Conference on Artificial Intelligence (IJCAI)</i>, Stockholm, Sweden, July 13-19, 2018<br>
            [<strong><font color="red">Most Influential IJCAI Papers (<u><a href="https://www.paperdigest.org/2022/05/most-influential-ijcai-papers-2022-05/" style="color:#2E41DC;">Top-6</a></u>)
            </strong></font>, Accept Rate = 6/710 = 0.8% | <strong><font color="red">Oral</strong></font>]<br>
            <a href="https://arxiv.org/abs/1805.10421">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2018][IJCAI]Emeasure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/E-measure">[Code]</a>
            <a href="https://drive.google.com/file/d/10c_PU4pa-7KJ-c2puLfTRE0ZBWgPBwiS/view?usp=sharing">[Meta-measure]</a>
            <a href="https://dengpingfan.github.io/ppts/Emeasure-IJCAI2018.pptx">[PPT]</a>
            <a href="https://drive.google.com/file/d/18tV4Fn8SZrVA5GunPpGbM_uabzDEEayZ/view">[Dataset]</a>
            <a href="https://www.ijcai.org/proceedings/2018/97">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Structure-measure: A New Way to Evaluate Foreground Maps</b><br>
            <u>Deng-Ping Fan</u>, Ming-Ming Cheng*, Yun Liu, Tao Li, Ali Borji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Venice, Italy, October 22-29, 2017<br>
            [<strong><font color="red">Spotlight</font></strong>, Accept rate = 56/2143 = 2.6%]<br>
            <a href="https://arxiv.org/abs/1708.00786">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2017][ICCV]S-measure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://drive.google.com/file/d/10c_PU4pa-7KJ-c2puLfTRE0ZBWgPBwiS/view?usp=sharing">[Meta-measure]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2017_S-measure_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html">[Official Version]</a>
          </p>
        </li> 
        
        <br><a><strong>Models</strong></a><br>
        <li>
          <p>
            <b>Masked Vision-Language Transformer in Fashion</b><br>
            Ge-Peng Ji^, Mingcheng Zhuge^, Dehong Gao, <u>Deng-Ping Fan*</u>, Christos Sakaridis, Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2023. (中国国际影响力优秀学术期刊)<br>
            <a href="https://arxiv.org/abs/2210.15110">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][MIR]MVLT_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/MVLT">[Code]</a>
            <!--<a href="">[Official Version]</a>-->
          </p>
        </li>
        <li>
          <p>
            <b>Sequential Interactive Image Segmentation</b><br>
            Zheng Lin, Zhao Zhang, Zi-Yue Zhu, <u>Deng-Ping Fan*</u>, Xia-Lei Liu<br>
            <i>Computational Visual Media (CVMJ)</i>, 2023. (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            <a href="https://dengpingfan.github.io/papers/[2022][CVMJ]SIIS.pdf">[PDF]</a>            
            <a href="https://dengpingfan.github.io/papers/[2022][CVMJ]SIIS_Chinese.pdf">[中译版]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Semantic Edge Detection with Diverse Deep Supervision</b><br>
            Yun Liu, Ming-Ming Cheng*, <u>Deng-Ping Fan</u>, Le Zhang, Jia-Wang Bian, and Dacheng Tao<br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2022, 130: 179-198. (IF: 13.369)<br>
            <a href="https://arxiv.org/abs/1804.02864">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][IJCV]SEdge_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/yun-liu/DDS">[Code]</a>
            <a href="https://link.springer.com/article/10.1007/s11263-021-01539-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>PVT v2: Improved baselines with Pyramid Vision Transformer</b><br>
            Wenhai Wang*, Enze Xie, Xiang Li, <u>Deng-Ping Fan</u>, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao<br>
            <i>Computational Visual Media (CVMJ)</i>, 2022, 8 (3): 415-424. (IF: 4.127, 中国国际影响力优秀学术期刊, CCF T2, Accept rate ~=12.5%)<br>
            <a href="https://arxiv.org/abs/2106.13797">[PDF]</a>
            <!--<a href="">[中译版]</a>-->
            <a href="https://github.com/whai362/PVT">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>-->
            <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Kaleido-BERT: Vision-Language Pre-training on Fashion Domain</b><br>
            Mingchen Zhuge^, Dehong Gao^, <u>Deng-Ping Fan*</u>, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, Vitural, June 19-25, 2021<br>
            <a href="https://arxiv.org/abs/2103.16110">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][CVPR]KaleidoBERT_Chinese.pdf">[中译版]</a>
            <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_609cd21de4b0fe322012dd28/3">[1小时解读]</a>
            <a href="https://github.com/mczhuge/Kaleido-BERT">[Code]</a>
            <a class="github-button" href="https://github.com/mczhuge/Kaleido-BERT" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhuge_Kaleido-BERT_Vision-Language_Pre-Training_on_Fashion_Domain_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</b><br>
            Wenhai Wang, Enze Xie, Xiang Li, <u>Deng-Ping Fan*</u>, Kaitao Song, Ding Liang, Tong Lu*, Ping Luo, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, Vitural, October 11-17, 2021<br>
            A pure Transformer backbone for dense prediction, such as object detection and semantic segmentation.<br>
            [<strong><font color="red">Most Influential ICCV2021 Papers 
            (<u><a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/" style="color:#2E41DC;">Top-2</a></u>)
            </strong></font>, Rate = 10/1617 = 0.62% | <strong><font color="red">Oral</strong></font>, Accept rate = 210/6236 = 3.4%]<br>
            <a href="https://arxiv.org/abs/2102.12122">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]PVT_Chinese.pdf">[中译版]</a>
            <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
            <a href="https://dengpingfan.github.io/papers/[2021][ICCV]PVT-ppt.pdf">[PPT]</a>
            <a href="https://github.com/whai362/PVT">[Code]</a>
            <!--<img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>-->
            <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        
        <br><a><strong>Benchmarks</strong></a><br>
        <li>
          <p>
            <b>Highly Accurate Dichotomous Image Segmentation</b><br>
            Xuebin Qin, Hang Dai, Xiaobin Hu, <u>Deng-Ping Fan*</u>, Ling Shao, Luc Van Gool<br>
            <i>European Conference on Computer Vision (ECCV)</i>, Tel Aviv, Israel, October 23-27, 2022<br>
            <a href="https://arxiv.org/pdf/2203.03041.pdf">[PDF]</a>
            <a href="https://dengpingfan.github.io/papers/[2022][ECCV]DIS_Chinese.pdf">[中译版]</a>
            <a href="https://xuebinqin.github.io/dis/index.html">[Project Page]</a>
            <a href="https://github.com/xuebinqin/DIS">[Code]</a>
            <a class="github-button" href="https://github.com/xuebinqin/DIS" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://replicate.com/arielreplicate/dichotomous_image_segmentation">[Online Demo]</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-031-19797-0_3">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Facial-Sketch Synthesis: A New Challenge</b><br>
            <u>Deng-Ping Fan</u>, Ziling Huang, Peng Zheng, Hong Liu*, Xuebin Qin*, Luc Van Gool<br>
            <i>Machine Intelligence Research (MIR)</i>, 2022, 19 (4): 257-287. (中国国际影响力优秀学术期刊)<br>
            <a href="https://arxiv.org/abs/2112.15439">[PDF]</a> 
            <a href="https://dengpingfan.github.io/papers/[2022][MIR]FS2K_Chinese.pdf">[中译版]</a>
            <a href="https://youtu.be/COhUly9REQc">[Video]</a>
            <a href="https://pan.baidu.com/s/1rmE8iJeurizlAzKmOAMakg">[MIR报告PPT (百度盘提取码eoa3)]</a>
            <a href="https://www.bilibili.com/video/BV1jP411V7bx/">[视频解读]</a>
            <a href="https://github.com/DengPingFan/FSGAN">[Code]</a>
            <a class="github-button" href="https://github.com/DengPingFan/FSGAN" data-icon="octicon-star" data-show-count="true">Star</a>
            <a href="https://github.com/DengPingFan/FS2KToolbox">[Evaluation Tool]</a>
            <a href="https://github.com/DengPingFan/FS2K">[FS2K dataset]</a>
            <a href="https://github.com/DengPingFan/FaceSketch-Awesome-List">[Awesome List]</a>
            <a href="https://www.mi-research.net/en/article/doi/10.1007/s11633-022-1349-9">[Official Version]</a>
          </p>
        </li>
      </ul>
      
      <!-- Patents -->
      <h2> 
        <a id="Patents" class = "anchor" href="#Patents" >Patents</a>
      </h2>
      <ul>
        <li>
          <p>
            US Patent App. 17/186,745, 2022, <a href="https://patentimages.storage.googleapis.com/0b/bd/b0/30f3936eed045e/US20220277218A1.pdf">Domain Specific Pre-training of Cross Modality Transformer Model</a>
          </p>
        </li>
        <li>
          <p>
            US Patent App. 17/475,099, 2022, <a href="https://patentimages.storage.googleapis.com/98/6c/15/49188c6b4910f3/US20220253639A1.pdf">Complementary Learning for Multi-modal Saliency Detection</a>
          </p>
        </li>
        <li>
          <p>
            US Patent App. 17/336,748, 2022, <a href="https://patentimages.storage.googleapis.com/6d/57/ba/9aae39675c7f00/US20220230324A1.pdf">Camouflaged Object Segmentation Method with Distraction Mining</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL202110180500.8, 2022, <a href="http://www.soopat.com/Patent/202110180500">Grouping reverse attention-based camouflage object detection method and system</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL202110078735.6, 2022, <a href="http://www.soopat.com/Patent/202110078735">Camouflage target image segmentation method based on information mining</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL202110160367.X, 2022, <a href="http://www.soopat.com/Patent/202110160367">Cooperative saliency object detection method and system based on collaborative learning</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL201910347420.X, 2022, <a href="http://www.soopat.com/Patent/201910347420">Video saliency object detection method based on attention transfer mechanism</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL201810173285.7, 2022, <a href="http://www.soopat.com/Patent/201810173285">Deep network-based universal detection method for multiple types of tasks</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL201911416282.2, 2021, <a href="http://www.soopat.com/Patent/201911416282">RGBD salient object detection method based on twin network</a>
          </p>
        </li>
        <li>
          <p>
            Chinese Patent App. ZL201810171102.8, 2021, <a href="http://www.soopat.com/Patent/201810171102">Binary-system-based foreground image similarity evaluation method</a>
          </p>
        </li>
      </ul>

      <div id="footer">
        <div id="footer-text"></div>
      </div>
      © Deng-Ping Fan

      <div class="container">
         <div style="display:inline-block;width:200px;">
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5d4snb3frum&amp;s=220&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
        </div>
      </div>

    </div>
  </body>
</html>
