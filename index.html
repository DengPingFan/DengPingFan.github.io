<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Deng-Ping Fan&#39;s home page">
    <link rel="shortcut icon" href="./images/logo-ethz2.png">
    <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="./assets/jemdoc.css" type="text/css">
    
    <!--<link href="static/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="static/xin.css" rel="stylesheet">-->
    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88572407-1', 'auto');
      ga('send', 'pageview');
    </script>
    <meta name="google-site-verification" content="F0Q0t5oLq1pGwXGMf_38oA2MxW_zfiMRsQTYD4_GJoQ"/>
    <title>Deng-Ping Fan</title>
  </head>

  <body>
    
    <div id="layout-content" style="margin-top:25px">
      <table>
        <tbody>
          <tr>
            <td width="670">
              <div id="toptitle">
                <h1>Deng-Ping Fan &nbsp; 范登平</h1>
              </div>
              
              <div class="navbar-collapse collapse">
                <h2>
                <!--<a href="index.html">Home|</a>
                <a href="publications/index.htm">Publications|</a>
                <a href="projects/index.htm">Projects|</a>
                <a href="team/index.htm">Team|</a>
                <a href="teaching/index.htm">Teaching|</a>
                <a href="service/index.htm">Service</a>-->

                <a href="index.html">Home|</a>
                <a href="#Activities">Activities|</a>
                <a href="#Publications">Publications|</a>
                <a href="#Awards">Awards|</a>    
                <a href="#Acceptance">Acc.Rate</a>   
              </h2>
            </div>    
              <p>
                Deng-Ping Fan is a Postdoctoral Researcher, working with <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html"><u>Prof. Luc Van Gool</u></a> in <a href="https://vision.ee.ethz.ch/"><u>Computer Vision Lab</u></a> @ <a href="https://ethz.ch/en.html"><u>ETH Zurich</u></a>.
                From 2019-2021, he was a research scientist (PI) and team lead of IIAI-CV&Med in <a href="http://www.inceptioniai.org/"><u>IIAI</u></a>.
                He received his PhD. from Nankai University in 2019 under the supervision of <a href="https://mmcheng.net/cmm/"><u>Prof. Ming-Ming Cheng</u></a>.
                His research interests includes computer vision, image processing, medical image analysis, <i>etc.</i>
              </p>
              <p>
                范登平，苏黎世联邦理工大学(ETH Zurich)博士后，
                曾担任IIAI研究员，阿里达摩院高级算法工程师。
                在CCF A类顶级国际期刊和会议上发表学术论文25 篇，包括5篇IEEE T-PAMI，
                五次获得CVPR/ICCV/IJCAI oral，连续两年获CVPR最佳论文提名奖（入选率为0.8%和0.4%），
                博士论文获得CCF优秀博士学位论文奖，谷歌学术总引用4200余次。
              </p>
              <h3 style="padding-top:-5px"></h3>
              <object id="object" data="assets/envelope.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="mailto:dengpfan@gmail.com">dengpfan@gmail.com</a>
              &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
              <object id="object" data="assets/scholar.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=zh-CN&oi=ao" target="_blank">Google Scholar</a>
              &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
              <object id="object" data="assets/github.svg" width="15" height="15" type="image/svg+xml"></object> &nbsp;
              <a href="https://github.com/DengPingFan" target="_blank">Github</a>
            </td>
            <td>
              <img src="./images/2-inch.jpg" border="0" width="150">
            </td>
          </tr>
        </tbody>
      </table>
      
       
      <!-- Research Interests -->
      <h2> 
        <a id="Interests" class="anchor" href="#Interests">Research Interests</a> 
      </h2>
      <ul>
        <li>
          <p>
           <strong>Camouflaged Scene Understanding:</strong> Concealed Object Detection (COD), Transparent Object Detection, Defect Detection, Abnormal Detection<br>
          </p>
        </li>
        <li>
          <p>
           <strong>Multi-Modal AI:</strong> Vision-Language, Vision-Audio, Vision-Language-Audio<br>
          </p>
        </li>
        <li>
          <p>
           <strong>Medical Image Processing:</strong> Hippocampal segmentation, Polyp Segmentation, Lung Infection Segmentation<br>
          </p>
        </li>
        <li>
          <p>
           <strong>Visual Attention:</strong> Image Salient Object Detection (ISOD), RGB-D SOD, RGB-T SOD, Remote Sensing SOD, High-Resolusion SOD (HRSOD), Lidar/Point Cloud SOD, CoSOD, Salient Instance Detection (SID), Salient Object Ranking (SOR), Video Salient Object Detection (VSOD), Visual-auditory SOD<br>
          </p>
        </li>
        <li>
          <p>
           <strong>3D Vision:</strong> 3D reconstruction, AR/VR, Depth estimation, Light Field SOD (LFSOD), RGB-D SOD<br>
          </p>
        </li>
      </ul>
        
      <!-- News -->
      <h2> 
        <a id="News" class="anchor" href="#News">News</a> 
      </h2>
      <ul>
        <li>
          <p>
            (2021/12), HUAWEI 藤蔓技术论坛2021 Talk: <b>“伪装目标检测技术与应用”</b>, 2021. <a href="http://dpfan.net/wp-content/uploads/IIAI-伪装目标检测技术与应用-V6.pptx"><u>link</u></a>/<u><a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_60a36389e4b0adb2d8652c35/3">video</a></u><br>
          </p>
        </li>
        <li>
          <p>
            (2021/05), 机器之心走近全球顶尖实验室系列: <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/page/1827300?navIndex=3"><u>起源人工智能研究院(IIAI)</u>.</a><br>
          </p>
        </li>
           <li>
          <p>
            (2020/09), SOD Leaderboards (RGB SOD: <a href="https://paperswithcode.com/task/salient-object-detection"><u>link</u></a>; RGB-D SOD: <a href="https://paperswithcode.com/task/rgb-d-salient-object-detection"><u>link</u></a>; Light Field SOD: <a href="https://github.com/kerenfu/LFSOD-Survey"><u>link</u></a>; Video SOD: <a href="https://paperswithcode.com/task/video-salient-object-detection"><u>link</u><a>; Co-SOD: <a href="https://paperswithcode.com/task/co-saliency-detection"><u>link</u></a>) have been released.<br>
          </p>
        </li>
      </ul>
      
      <!-- Activities -->
      <h2> 
        <a id="Activities" class="anchor" href="#Activities">Activities</a> 
      </h2>
      <ul>
        <li>
          <p>
            <b>Area Chair/Senior Programme Committee: </b>IJCAI 2021, MICCAI 2020 workshop (OMIA7)
            <br>
          </p>
        </li>
        <li>
          <p>
            <b>Conference Program Committee Board: </b>IJCAI (2022-2024)
            <br>
          </p>
        </li>  
        <li>
          <p>
            Conference Programme Committee: ICML 2022, ICLR 2022, CVPR 2019-2022, ICCV 2019、2021, ECCV 2020, AAAI 2018、2020-2022, <i>etc</i>. 
            <br>
          </p>
        </li>
        <li>
          <p>
            Journal Reviewer: IEEE TPAMI, IEEE TIP, IEEE TNNLS, IEEE TMM, IEEE TMI, IJCV, MIA, <i>etc</i>.
            <br>
          </p>
        </li>
        <li>
          <p>
            Memberships: Chinese Institute of Electronics (<a href="https://www.cie-info.org.cn/">CIE</a>) <a href="https://www.cie-info.org.cn/site/content/4286.html" style="color:#2E41DC;"><u>Senior Member</u></a>, <i>etc</i>.
            <br>
          </p>
        </li>
        <li>
          <p>
            Co-organize <a href="https://jingzhang617.github.io/uncertaintyestimation_iccv21.github.io/" style="color:#2E41DC;"><u>Uncertainty Estimation for Dense Prediction Tasks</u></a> at ICCV 2021.
            <br>
          </p>
        </li>
      </ul>
      
      <!-- arXiv -->
      <h2> 
        <a id="Publications" class = "anchor" href="#Publications">arXiv (*: Corresponding, ^Equal contributions)</a>
      </h2>
      <ul>
        <li>
          <p>
            <b>Deep Facial Synthesis: A New Challenge</b><br>
            <strong>Deng-Ping Fan</strong>, Ziling Huang, Peng Zheng, Hong Liu*, Xuebin Qin*, Luc Van Gool<br>
            <i>arXiv</i>, 2021<br>
            <a href="https://arxiv.org/abs/2112.15439">[PDF]</a>
            <a href="https://github.com/DengPingFan/FSGAN">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Salient Objects in Clutter</b><br>
            <strong>Deng-Ping Fan</strong>, Jing Zhang, Gang Xu, Ming-Ming Cheng*, Ling Shao<br>
            <i>arXiv</i>, 2021<br>
            [Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_12" style="color:#2E41DC;"><u>ECCV 2018</u></a>]<br>
            <a href="https://arxiv.org/abs/2105.03053">[PDF]</a>
            <a href="https://github.com/DengPingFan/SOC-DataAug">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Boundary-aware segmentation network for mobile and web applications</b><br>
            Xuebin Qin, <strong>Deng-Ping Fan*</strong>, Chenyang Huang, Cyril Diagne, Zichen Zhang, Adrià Cabeza Sant'Anna, Albert Suarez, Martin Jagersand, Ling Shao<br>
            <i>arXiv</i>, 2021<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html" style="color:#2E41DC;"><u>CVPR 2019</u></a>]<br>
            <a href="https://arxiv.org/abs/2101.04704">[PDF]</a>
            <a href="https://cloud.tencent.com/developer/article/1688893">[中文解读]</a>
            <a href="https://github.com/xuebinqin/BASNet">[Code]</a>
            <img src="https://img.shields.io/github/stars/xuebinqin/BASNet?style=social"/>
            <a href="https://www.youtube.com/watch?v=VxJmS8avjbY">[Tutorial (4k views)]</a>
            <a href="https://weibo.com/tv/show/1034:4500977827381257?from=old_pc_videoshow">[Webo (1,070k views)]</a>
            <a href="https://github.com/xuebinqin/BASNet">[Demo]</a>
            <img src="https://img.shields.io/github/stars/cyrildiagne/ar-cutpaste?style=social"/>
            <a href="http://dpfan.net/wp-content/uploads/SOD%E5%BA%94%E7%94%A8BASNetAR%E8%B4%B4%E5%9B%BE%E5%BA%94%E7%94%A8.mp4">[Video]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers</b><br>
            Bo Dong, Wenhai Wang, <strong>Deng-Ping Fan*</strong>, Jinpeng Li, Huazhu Fu, Ling Shao<br>
            <i>arXiv</i>, 2021<br>
            <a href="https://arxiv.org/abs/2108.06932">[PDF]</a>
            <a href="https://github.com/DengPingFan/Polyp-PVT">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/Polyp-PVT?style=social"/>
          </p>
        </li>
        
        <li>
          <p>
            <b>Salient Object Detection via Integrity Learning</b><br>
            Mingchen Zhuge^, <strong>Deng-Ping Fan^</strong>, Nian Liu, Dingwen Zhang*, Dong Xu, Ling Shao<br>
            <i>arXiv</i>, 2021<br>
            <a href="https://arxiv.org/abs/2101.07663">[PDF]</a>
            <a href="https://github.com/mczhuge/ICON">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Weakly Supervised Visual-Auditory Saliency Detection with Multigranularity Perception</b><br>
            Guotao Wang, Chenglizhao Chen*, <strong>Deng-Ping Fan</strong>, Aimin Hao, Hong Qin<br>
            <i>arXiv</i>, 2021<br>
            [Extension of <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_From_Semantic_Categories_to_Fixations_A_Novel_Weakly-Supervised_Visual-Auditory_Saliency_CVPR_2021_paper.html" style="color:#2E41DC;"><u>CVPR 2021</u></a>]<br>
            <a href="https://arxiv.org/abs/2112.13697">[PDF]</a>
            <a href="https://github.com/guotaowang/STANet">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Generative Transformer for Accurate and Reliable Salient Object Detection</b><br>
            Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai*, Aixuan Li, Yunqiu Lv, Xinyu Tian, <strong>Deng-Ping Fan</strong> and Nick Barnes<br>
            <i>arXiv</i>, 2021<br>
            <a href="https://github.com/fupiao1998/TrasformerSOD">[Code]</a>
          </p>
        </li>
        <li>
          <p>
            <b>IC9600: A Benchmark Dataset for Automatic Image Complexity Assessment</b><br>
            Yingjie Zhai, Tinglei Feng, Jufeng Yang, Jie Liang, <strong>Deng-Ping Fan</strong>, Jing Zhang, Ling Shao, and Dacheng Tao<br>
            <i>arXiv</i>, 2021<br>
            <a href="">[PDF]</a>
          </p>
        </li>
      </ul>
      
      <!-- Journal papers -->
      <h2> Selected Journal Publications</h2>
      <ul>
        <br><a><strong>In the Year of 2022</strong></a><br>
        <li>
          <p>
            <b>Concealed Object Detection</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Ming-Ming Cheng*, Ling Shao<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022<br>
            [<strong><font color="red"><a href="papers/SINet-V2-Award.pdf"><u>JDC 2021</u></a> Distinguish Paper</font></strong> | Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2102.10274">[PDF]</a>
            <a href="papers/[2021][PAMI]SINetV2_Chinese.pdf">[中译版]</a>
            <a href="http://mmcheng.net/cod/">[Project Page]</a> 
            <a href="papers/[2022][TPAMI]ConcealedOD_supp.pdf">[Supplementary Material]</a>
            <a href="https://github.com/GewelsJI/SINet-V2">[Code-Python]</a>
            <a href="https://cg.cs.tsinghua.edu.cn/jittor/news/2021-06-11-00-00-cod/">[Code-Jittor]</a>
            <a href="https://ieeexplore.ieee.org/document/9444794">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Re-thinking co-salient object detection</b><br>
            <strong>Deng-Ping Fan</strong>, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng*, Huazhu Fu, Jianbing Shen<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2022<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2007.03380v4">[PDF]</a>
            <a href="papers/[2021][TPAMI]CoSOD3k_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/CoEGNet?utm_source=catalyzex.com">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9358006">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Semantic Edge Detection with Diverse Deep Supervision</b><br>
            Yun Liu, Ming-Ming Cheng, <strong>Deng-Ping Fan</strong>, Le Zhang, Jia-Wang Bian, and Dacheng Tao<br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2022<br>
            <a href="https://arxiv.org/abs/1804.02864">[PDF]</a>
            <a href="papers/[2021][IJCV]SEdge_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/yun-liu/DDS">[Code]</a>
            <a href="https://link.springer.com/article/10.1007/s11263-021-01539-8">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>In the Year of 2021</strong></a><br>
        <li>
          <p>
            <b>Structure-measure: A new way to evaluate foreground maps</b><br>
            Ming-Ming Cheng*, <strong>Deng-Ping Fan</strong><br>
            <i>International Journal of Computer Vision (IJCV)</i>, 2021<br>
            [Extension of <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html" style="color:#2E41DC;"><u>ICCV 2017</u></a>]<br>
            <a href="papers/[2022][IJCV]Structure-Measure A New WaytoEvaluateForegroundMaps.pdf">[PDF]</a>
            <a href="papers/[2021][IJCV]Smeasure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://link.springer.com/article/10.1007/s11263-021-01490-8">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Siamese Network for RGB-D Salient Object Detection and Beyond</b><br>
            Keren Fu, <strong>Deng-Ping Fan*</strong>, Ge-Peng Ji, Qijun Zhao, Jianbing Shen, Ce Zhu<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2021<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u>]</a><br>
            <a href="https://arxiv.org/abs/2008.12134">[PDF]</a>
            <a href="papers/[2021][TPAMI]JLDCF_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/JLDCF/?utm_source=catalyzex.com">[Code]</a>
            <a href="https://ieeexplore.ieee.org/document/9406382">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Uncertainty Inspired RGB-D Saliency Detection</b><br>
            Jing Zhang, <strong>Deng-Ping Fan*</strong>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Sadegh Aliakbarian, Nick Barnes<br>
            <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2021<br>
            [Extension of <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html" style="color:#2E41DC;"><u>CVPR 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2009.03075">[PDF]</a>
            <a href="https://github.com/JingZhang617/UCNet?utm_source=catalyzex.com">[Code]</a>
            <img src="https://img.shields.io/github/stars/JingZhang617/UCNet?style=social"/>
            <a href="https://ieeexplore.ieee.org/document/9405467">[Official Version]</a>
          </p>
        </li>
        
        <li>
          <p>
            <b>Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks</b><br>
            <strong>Deng-Ping Fan</strong>, Zheng Lin, Zhao Zhang, Menglong Zhu, Ming-Ming Cheng*<br>
            <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</i>, 2021<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong>]<br>
            <a href="https://arxiv.org/abs/1907.06781">[PDF]</a>
            <a href="papers/[2021][TNNLS]SIPBenchmark_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/D3NetBenchmark">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/D3NetBenchmark?style=social"/>
            <a href="https://ieeexplore.ieee.org/abstract/document/9107477">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>认知规律启发的物体分割评价标准及损失函数</b><br>
            <b>Cognitive Vision Inspired Object Segmentation Metric and Loss Function</b><br>
            <strong>范登平</strong>， 季葛鹏， 秦雪彬， 程明明*<br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Xuebin Qin, Ming-Ming Cheng*<br>
            <i>中国科学：信息科学</i>, 2021<br>
            <i>SCIENTIA SINICA Informationis (SSI)</i>, 2021<br>
            [Extension of <a href="https://www.ijcai.org/proceedings/2018/97" style="color:#2E41DC;"><u>IJCAI 2018</u></a>]<br>
            <a href="papers/[2021][SSI]EmeasureEng.pdf">[PDF]</a>
            <a href="https://github.com/GewelsJI/Hybrid-Eloss/">[Code]</a>
            <a href="https://www.sciengine.com/publisher/scp/journal/SSI/51/9/10.1360/SSI-2020-0370?slug=fulltext">[Official Version]</a>
          </p>
        </li>
        
        <li>
          <p>
            <b>JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation</b><br>
            Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, <strong>Deng-Ping Fan</strong>, Rong-Guo Zhang, Ming-Ming Cheng*<br>
            <i>IEEE Transactions on Image Processing (TIP)</i>, 2021<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong>]<br>
            <a href="https://arxiv.org/abs/2004.07054">[PDF]</a>
            <a href="papers/[2021][TIP]JCS_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/yuhuan-wu/JCS">[Code]</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9357961">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>COVID-19 Lung Infection Segmentation with A Novel Two-Stage Cross-Domain Transfer Learning Framework</b><br>
            Jiannan Liu, Bo Dong, Shuai Wang, Hui Cui, <strong>Deng-Ping Fan</strong>, Jiquan Ma*, Geng Chen*<br>
            <i>Medical Image Analysis (MIA)</i>, 2021<br>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8342869/">[PDF]</a>
            <a href="https://github.com/Jiannan-Liu/nCoVSegNet">[Code]</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S1361841521002504?casa_token=4dIB3WQF8osAAAAA:WqK99Z08kXXrqrrX6DXK_6eWH_uX944wn_QaommMaJ45HItdj8Nc5EMS_DOUvjMTio476WmnLMI">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>In the Year of 2020</strong></a><br>
        <li>
          <p>
            <b>Inf-Net: Automatic Covid-19 Lung Infection Segmentation from CT Images</b><br>
            <strong>Deng-Ping Fan</strong>, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen*, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Transactions on Medical Imaging (TMI)</i>, 2020<br>
            [<strong><font color="red">ESI Highly Cited Paper (1%)</font></strong> | <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=42" style="color:#2E41DC;"><u>TMI 50 most frequently accessed articles</u></a> | Extension of <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26" style="color:#2E41DC;"><u>MICCAI 2020</u></a>]<br>
            <a href="https://arxiv.org/abs/2004.14133">[PDF]</a>
            <a href="papers/[2020][TMI]InfNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/Inf-Net?utm_source=catalyzex.com">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/Inf-Net?style=social"/>
            <a href="https://ieeexplore.ieee.org/document/9098956">[Official Version]</a>
          </p>
        </li>
        <!--
        <li>
          <p>
            <b>xxxx</b><br>
            xxxx<br>
            <i>xxxx</i>, xxxx<br>
            <a href="">[PDF]</a>
            <a href="">[Code]</a>
            <a href="">[Official Version]</a>
          </p>
        </li>
        </li>
        <li>
          <p>
            <b>xxxx</b><br>
            xxxx<br>
            <i>xxxx</i>, xxxx<br>
            <a href="">[PDF]</a>
            <a href="">[Code]</a>
            <a href="">[Official Version]</a>
          </p>
        </li> -->
      </ul>
      
      <!-- conference papers -->
      <h2> Selected Conference Publications</h2>
      <ul> 
        <br><a><strong>In the Year of 2021</strong></a><br>
        <li>
          <p>
            <b>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</b><br>
            Wenhai Wang, Enze Xie, Xiang Li, <strong>Deng-Ping Fan*</strong>, Kaitao Song, Ding Liang, Tong Lu*, Ping Luo, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2021<br>
            [<strong><font color="red">ICCV2021 <u><a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/" style="color:#2E41DC;">Top-10</a></u> Influential Papers (Accept rate = 10/6236 = 0.16%) & Oral</strong> (Accept rate = 210/6236 = 3.4%) </font>]<br>
            [ <br>
            <a href="https://arxiv.org/abs/2102.12122">[PDF]</a>
            <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
            <a href="papers/[2021][ICCV]PVT-ppt.pdf">[PPT]</a>
            <a href="https://github.com/whai362/PVT">[Code]</a>
            <img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>RGB-D saliency detection via cascaded mutual information minimization</b><br>
            Jing Zhang, <strong>Deng-Ping Fan*</strong>, Yuchao Dai, Xin Yu, Yiran Zhong, Nick Barnes, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2021<br>
            <a href="https://arxiv.org/abs/2109.07246">[PDF]</a>
            <a href="https://github.com/JingZhang617/cascaded_rgbd_sod">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_RGB-D_Saliency_Detection_via_Cascaded_Mutual_Information_Minimization_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Specificity-preserving RGB-D Saliency Detection</b><br>
            Tao Zhou, <strong>Deng-Ping Fan*</strong>, Geng Chen, Yi Zhou, Huazhu Fu<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2021<br>
            <a href="https://arxiv.org/abs/2108.08162">[PDF]</a>
            <a href="https://github.com/taozh2017/SPNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_Specificity-Preserving_RGB-D_Saliency_Detection_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Full-duplex strategy for video object segmentation</b><br>
            Ge-Peng Ji, <strong>Deng-Ping Fan*</strong>, Keren Fu, Zhe Wu, Jianbing Shen, Ling Shao<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2021<br>
            <a href="https://arxiv.org/abs/2108.03151">[PDF]</a>
            <a href="papers/[2021][ICCV]VSOD_FSNet_Chinese.pdf">[中译版]</a>        
            <a href="https://github.com/GewelsJI/FSNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ji_Full-Duplex_Strategy_for_Video_Object_Segmentation_ICCV_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Kaleido-BERT: Vision-Language Pre-training on Fashion Domain</b><br>
            Mingchen Zhuge, Dehong Gao, <strong>Deng-Ping Fan*</strong>, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021<br>
            <a href="https://arxiv.org/abs/2103.16110">[PDF]</a>
            <a href="papers/[2021][CVPR]KaleidoBERT_Chinese.pdf">[中译版]</a>
            <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_609cd21de4b0fe322012dd28/3">[1小时解读]</a>
            <a href="https://github.com/mczhuge/Kaleido-BERT">[Code]</a>
            <img src="https://img.shields.io/github/stars/mczhuge/Kaleido-BERT?style=social"/>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhuge_Kaleido-BERT_Vision-Language_Pre-Training_on_Fashion_Domain_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Group Collaborative Learning for Co-Salient Object Detection</b><br>
            Qi Fan, <strong>Deng-Ping Fan*</strong>, Huazhu Fu, Chi-Keung Tang, Ling Shao, Yu-Wing Tai<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021<br>
            <a href="https://arxiv.org/abs/2104.01108">[PDF]</a>
            <a href="https://github.com/fanq15/GCoNet">[Code]</a>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Group_Collaborative_Learning_for_Co-Salient_Object_Detection_CVPR_2021_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Progressively Normalized Self-Attention Network for Video Polyp Segmentation</b><br>
            Ge-Peng Ji^, Yu-Cheng Chou^, <strong>Deng-Ping Fan*</strong>, Geng Chen, Huazhu Fu, Debesh Jha, Ling Shao<br>
            <i>Medical Image Computing and Computer Assisted Intervention (MICCAI)</i>, 2021<br>
            [<strong><font color="red">Early Accept & Student Travel Award</font></strong>, Accept rate = 13%]<br>
            <a href="https://arxiv.org/abs/2105.08468">[PDF]</a>
            <a href="papers/[2021][MICCAI]PNSNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/GewelsJI/PNS-Net">[Code]</a>
            <a href="https://miccai2021.org/openaccess/paperlinks/2021/09/01/378-Paper0320.html">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>In the Year of 2020</strong></a><br>
        <li>
          <p>
            <b>PraNet: Parallel reverse attention network for polyp segmentation</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu*, Jianbing Shen*, Ling Shao<br>
            <i>Medical Image Computing and Computer Assisted Intervention (MICCAI)</i>, 2020<br>
            [<strong><font color="red">Early accept & Oral</font></strong>, Accept rate = 13% | <strong><a href="Most Influential Paper Award"><u>JDC 2021</u></a><font color="red"> Most Influential (Application) Paper</font></strong> | <strong><font color="red">Best precision</font> of MediaEval 2020 Workshop at Medico Track</strong>]<br>
            <a href="https://arxiv.org/abs/2006.11392">[PDF]</a>
            <a href="papers/[2020][MICCAI]PraNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/PraNet">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/PraNet?style=social"/>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network</b><br>
            <strong>Deng-Ping Fan^</strong>, Yingjie Zhai^, Ali Borji, Jufeng Yang*, Ling Shao<br>
            <i>European Conference on Computer Vision (ECCV)</i>, 2020<br>
            <a href="https://arxiv.org/abs/2007.02713">[PDF]</a>
            <a href="papers/[2020][ECCV]BBSNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/zyjwuyan/BBS-Net">[Code]</a>
            <img src="https://img.shields.io/github/stars/zyjwuyan/BBS-Net?style=social"/>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-58610-2_17">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection</b><br>
            Keren Fu, <strong>Deng-Ping Fan*</strong>, Ge-Peng Ji, Qijun Zhao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020<br>
            <a href="https://arxiv.org/abs/2004.08515">[PDF]</a>
            <a href="papers/[2020][CVPR]JLDCF_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/kerenfu/JLDCF/">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>UC-Net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders*</b><br>
            Jing Zhang, <strong>Deng-Ping Fan*</strong>, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, Nick Barnes<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020<br>
            [<strong><font color="red">Best Paper Nomination & Oral</font></strong>, Accept rate 0.44%]<br>
            <a href="https://arxiv.org/abs/2004.05763">[PDF]</a>
            <a href="papers/[2020][CVPR]UCNet_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/JingZhang617/UCNet">[Code]</a>
            <img src="https://img.shields.io/github/stars/JingZhang617/UCNet?style=social"/>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Taking a deeper look at co-salient object detection</b><br>
            <strong>Deng-Ping Fan^</strong>, Zheng Lin^, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, Ming-Ming Cheng*<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020<br>
            <a href="">[PDF]</a>
            <a href="papers/[2020][CVPR]CoSOD3k_Chinese.pdf">[中译版]</a>
            <a href="CoSalBenchmark-EvaluationTools.zip">[Evaluation Code]</a>
            <a href="https://pan.baidu.com/s/1FQZyg39it7pSoXrSa9k6VA">[CoSOD3K (Baidu: ky1v)]</a>
             <a href="https://drive.google.com/file/d/1nv20lZMMhHEOe9_elDHy2motgMtejCZM/view?usp=sharing">[CoSOD3K (Google)]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Taking_a_Deeper_Look_at_Co-Salient_Object_Detection_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Camouflaged object detection</b><br>
            <strong>Deng-Ping Fan</strong>, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen*, Ling Shao<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020<br>
            [<strong><font color="red">Oral</font></strong>, Accept rate = 335/5865 = 5.7% | <strong><font color="red">Coverage of 《<a href="https://www.newscientist.com/">New scientist</a>》</font></strong>]<br>
            <a href="">[PDF]</a>
            <a href="papers/[2020][CVPR]COD_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/SINet/">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/SINet?style=social"/>
            <a href="http://mc.nankai.edu.cn/cod">[Demo]</a>
            <a href="https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_60a36389e4b0adb2d8652c35/3">[1小时解读]</a>
            <a href="http://dpfan.net/wp-content/uploads/1052-oral.mp4">[Video]</a>
            <a href="https://pan.baidu.com/s/1EtH2tUdbBt16w5dgve7JhQ">[2.25G COD10K_All (Baidu: w3up)]</a>
            <a href="https://drive.google.com/file/d/1vRYAie0JcNStcSwagmCq55eirGyMYGm5/view?usp=sharing">[2.25G COD10K_All (Google)]</a>
            <a href="">[微信群 (WeChat: CVer222)]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html">[Official Version]</a>
          </p>
        </li>
        <br><a><strong>Before 2019</strong></a><br>
        <li>
          <p>
            <b>Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection</b><br>
            Jia-Xing Zhao^, Yang Cao^, <strong>Deng-Ping Fan^</strong>, Ming-Ming Cheng*, Xuan-Yi Li, Le Zhang<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019<br>
            <a href="">[PDF]</a>
            <a href="papers/[2019][CVPR]CPFP_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/rgbdsalpyr/">[Project Page]</a>
            <a href="https://github.com/JXingZhao/ContrastPrior">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Contrast_Prior_and_Fluid_Pyramid_Integration_for_RGBD_Salient_Object_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Shifting More Attention to Video Salient Object Detection</b><br>
            <strong>Deng-Ping Fan</strong>, Wenguan Wang, Ming-Ming Cheng*, Jianbing Shen<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2019<br>
            [<strong><font color="red">Best Paper Finalist & Oral</font></strong>, Accept rate 0.87%]<br>
            <a href="">[PDF]</a>
            <a href="papers/[2019][CVPR]SSAV_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/davsod/">[Project Page]</a>
            <a href="https://github.com/DengPingFan/DAVSOD">[Code]</a>
            <img src="https://img.shields.io/github/stars/DengPingFan/DAVSOD?style=social"/>
            <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Scoot: A Perceptual Metric for Facial Sketches</b><br>
            <strong>Deng-Ping Fan</strong>, ShengChuan Zhang, Yu-Huan Wu, Yun Liu, Ming-Ming Cheng*, Bo Ren, Paul Rosin, and Rongrong Ji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2019<br>
            <a href="https://arxiv.org/abs/1908.08433.pdf">[PDF]</a>
            <a href="papers/[2019][ICCV]Scoot_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/scoot/">[Project Page]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2019_Scoot_Supplementary.pdf">[Supplementary Material]</a>
            <a href="https://github.com/DengPingFan/Scoot">[Code]</a>
            <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.html">[Official Version]</a>
          </p>
        </li>
        <li>
          <p>
            <b>Enhanced-alignment measure for binary foreground map evaluation</b><br>
            <strong>Deng-Ping Fan</strong>, Cheng Gong, Yang Cao, Bo Ren*, Ming-Ming Cheng, Ali Borji<br>
            <i>International Joint Conference on Artificial Intelligence (IJCAI)</i>, 2018<br>
            [<strong><font color="red">IJCAI2018 <u><a href="https://www.paperdigest.org/2021/03/most-influential-ijcai-papers-2021-03/" style="color:#2E41DC;">Top-10</a></u> Influential Papers ((Rate = 10/710 = 1.4%)) & Oral</strong></font> (20%)]<br>
            <a href="https://arxiv.org/abs/1805.10421">[PDF]</a>
            <a href="papers/[2018][IJCAI]Emeasure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/E-measure">[Code]</a>
            <a href="https://www.ijcai.org/proceedings/2018/97">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Salient objects in clutter: Bringing salient object detection to the foreground</b><br>
            <strong>Deng-Ping Fan</strong>, Ming-Ming Cheng*, Jiang-Jiang Liu, Shang-Hua Gao, Qibin Hou, Ali Borji<br>
            <i>European Conference on Computer Vision (ECCV)</i>, 2018<br>
            <a href="https://arxiv.org/abs/1803.06091">[PDF]</a>
            <a href="papers/[2018][ECCV]SOCBenchmark_Chinese.pdf">[中译版]</a>
            <a href="https://mmcheng.net/socbenchmark/">[Project Page]</a>
            <a href="https://link.springer.com/chapter/10.1007/978-3-030-01267-0_12">[Official Version]</a>
          </p>
        </li> 
        <li>
          <p>
            <b>Structure-measure: A New Way to Evaluate Foreground Maps</b><br>
            <strong>Deng-Ping Fan</strong>, Ming-Ming Cheng*, Jiang-Jiang Liu, Shang-Hua Gao, Qibin Hou, Ali Borji<br>
            <i>International Conference on Computer Vision (ICCV)</i>, 2017<br>
            [<strong><font color="red">Spotlight</font></strong>, Accept rate = 56/2143 = 2.6%]<br>
            <a href="https://arxiv.org/abs/1708.00786">[PDF]</a>
            <a href="papers/[2017][ICCV]S-measure_Chinese.pdf">[中译版]</a>
            <a href="https://github.com/DengPingFan/S-measure">[Code]</a>
            <a href="https://yun-liu.github.io/materials/ICCV2017_S-measure_PPT.pdf">[PPT]</a>
            <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Fan_Structure-Measure_A_New_ICCV_2017_paper.html">[Official Version]</a>
          </p>
        </li> 
      </ul>
      
      <!-- Awards and Honors -->
      <h2> 
        <a id="Awards" class = "anchor" href="#Awards" >Awards and Honors</a>
      </h2>
      <ul> 
        <li>
          <p>
            <b>CCF Outstanding Doctoral Dissertation Award, 2021</b>
            <br>
          </p>
        </li>
        <li>
          <p>
            <b>吴文俊人工智能自然科学二等奖 (4/5), 2020</b>
            <br>
          </p>
        </li>
        <li>
          <p>
            <b>CVPR Best Paper Nomination, 2020</b>
            <br>
          </p>
        </li>
        <li>
          <p>
            <b>CVPR Best Paper Finalist, 2019</b>
            <br>
          </p>
        </li>
        <li>
          <p>
            Outstanding Reviewer: CVPR 2019 with special mention (25/2887), CVPR 2020 (141/3664), CVPR 2021 (1065/5246), ECCV 2020
            <br>
          </p>
        </li>
        <li>
          <p>
            Outstanding PhD. Graduates in Nankai University, 2019
            <br>
          </p>
        </li>
        <li>
          <p>
            Huawei PhD. Fellowship, 2017
            <br>
          </p>
        </li>
      </ul>
      
      <!-- Acceptance Rate -->
      <h2> 
        <a id="Acceptance" class = "anchor" href="#Acceptance" >Acceptance Rate</a>
      </h2>
      <ul> 
        <a>This list was created for personal interest. Sources of the information derive from official announcements.</a><br>
        <a>Refer:</a> 
        <a href="https://www.guide2research.com/topconf/">Guide2research</a>; 
        <a href="https://scholar.google.com/citations?view_op=top_venues&hl=zh-CN">Google Index</a>; 
        <a href="https://kesen.realtimerendering.com/">SIGGRAPH</a><br>
        <a href="papers/CCF-Recommend-2019.pdf">2019年CCF推荐国际学术会议和期刊目录.pdf</a><br>
        <a href="papers/TH-CPL-2019.pdf">2019年清华大学计算机学科群推荐学术会议和期刊列表.pdf</a><br>
        <a href="papers/CCF-ChineseJRecommend-2019.pdf">2019年CCF推荐中文科技期刊目录.pdf</a><br>
        <a>CVPR,ICCV,ECCV; ICLR,ICML,NIPS; AAAI,IJCAI;</a><br>
        <a><font color="red">Updated: 2022-02-02</font></a><br>
         <li>
          <p>
            <b>ICLR (2013–) 9月 h5: 150->203->253</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
              <tr>
              <th>year</th>
              <th>submitted</th>
              <th>oral</th>
              <th>poster</th>
              <th>total</th>
              </tr>
              </thead>
              <tbody>
              <tr>
              <td>2017</td>
              <td>507</td>
              <td>15(3.0%)</td>
              <td>183(36.1%)</td>
              <td>198(39.1%)</td>
              </tr>
              <tr>
              <td>2018</td>
              <td>981</td>
              <td>23(2.3%)</td>
              <td>291(29.7%)</td>
              <td>314(32.0%)</td>
              </tr>
              <tr>
              <td>2019</td>
              <td>1592</td>
              <td>24(1.5%)</td>
              <td>476(30.0%)</td>
              <td>500(31.4%)</td>
              </tr>
              <tr>
              <td>2020</td>
              <td>2594</td>
              <td>Oral-48(1.85%) Spotlight-108 (4.16%)</td>
              <td>531(20.5%)</td>
              <td>687(26.5%)</td>
              </tr>
              <tr>
              <td>2021</td>
              <td>2997</td>
              <td>
              <p>Outstanding-8 (0.27%)</p>
              <p>Oral-53(1.77%)</p>
              <p>Spotlight-114(3.9%)</p>
              </td>
              <td>693(23.1%)</td>
              <td>860(28.7%)</td>
              </tr>
              <tr>
              <td>2022</td>
              <td>3391</td>
              <td>
              <p>Outstanding-x (xx%)</p>
              <p>Oral-54(1.59%)</p>
              <p>Spotlight-176(5.19%)</p>
              </td>
              <td>865(25.5%)</td>
              <td>1095(32.3%)</td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>CVPR (1983–) (CCF A类） November (11月)  h5: 240->299->356</b><br><br>
              <table border="1" cellspacing="0" width="300">
              <thead>
              <tr>
              <th>year</th>
              <th>submitted</th>
              <th>oral+spotlight</th>
              <th>poster</th>
              <th>total</th>
              </tr>
              </thead>
              <tbody>
                <tr>
                <td>2000</td>
                <td>466</td>
                <td>66(14.2%)</td>
                <td>154(33.0%)</td>
                <td>220(47.2%)</td>
                </tr>
                <tr>
                <td>2001</td>
                <td>920</td>
                <td>78(8.5%)</td>
                <td>202(22.0%)</td>
                <td>280(30.5%)</td>
                </tr>
                <tr>
                <td>2003</td>
                <td>905</td>
                <td>60(6.6%)</td>
                <td>149(16.5%)</td>
                <td>209(23.1%)</td>
                </tr>
                <tr>
                <td>2004</td>
                <td>873</td>
                <td>54(6.2%)</td>
                <td>206(23.6%)</td>
                <td>260(29.8%)</td>
                </tr>
                <tr>
                <td>2005</td>
                <td>1160</td>
                <td>75(6.5%)</td>
                <td>251(21.6%)</td>
                <td>326(28.1%)</td>
                </tr>
                <tr>
                <td>2006</td>
                <td>1131</td>
                <td>54(4.8%)</td>
                <td>264(23.3%)</td>
                <td>318(28.1%)</td>
                </tr>
                <tr>
                <td>2007</td>
                <td>1250</td>
                <td>60(4.8%)</td>
                <td>293(23.4%)</td>
                <td>353(28.2%)</td>
                </tr>
                <tr>
                <td>2008</td>
                <td>1593</td>
                <td>64(4.0%)</td>
                <td>444(27.9%)</td>
                <td>508(31.9%)</td>
                </tr>
                <tr>
                <td>2009</td>
                <td>1464</td>
                <td>61(4.2%)</td>
                <td>323(22.0%)</td>
                <td>384(26.2%)</td>
                </tr>
                <tr>
                <td>2010</td>
                <td>1724</td>
                <td>78(4.5%)</td>
                <td>384(22.3%)</td>
                <td>462(26.8%)</td>
                </tr>
                <tr>
                <td>2011</td>
                <td>1677</td>
                <td>59(3.5%)</td>
                <td>377(22.5%)</td>
                <td>436(26.0%)</td>
                </tr>
                <tr>
                <td>2012</td>
                <td>1933</td>
                <td>48(2.5%)</td>
                <td>415(22.5%)</td>
                <td>463(24.0%)</td>
                </tr>
                <tr>
                <td>2013</td>
                <td>1870</td>
                <td>60(3.2%)</td>
                <td>411(22.0%)</td>
                <td>471(25.2%)</td>
                </tr>
                <tr>
                <td>2014</td>
                <td>1807</td>
                <td>104(5.8%)</td>
                <td>436(24.1%)</td>
                <td>540(29.9%)</td>
                </tr>
                <tr>
                <td>2015</td>
                <td>2123</td>
                <td>71(3.3%)</td>
                <td>531(25.0%)</td>
                <td>602(28.3%)</td>
                </tr>
                <tr>
                <td>2016</td>
                <td>2145</td>
                <td>82 (3.8%)+123(9.5%)</td>
                <td>438(20.4%)</td>
                <td>643(29.9%)</td>
                </tr>
                <tr>
                <td>2017</td>
                <td>2620</td>
                <td>71 (2.7%)+144(8.2%)</td>
                <td>568(21.7%)</td>
                <td>783(29.9%)</td>
                </tr>
                <tr>
                <td>2018</td>
                <td>3309</td>
                <td>70 (2.1%)+224(8.9%)</td>
                <td>685(20.7%)</td>
                <td>979(29.5%)</td>
                </tr>
                <tr>
                <td>2019</td>
                <td>5165</td>
                <td>45 best paper finalist (0.87%) + 288 oral (5.6%)</td>
                <td>1006(19.5%)</td>
                <td>1294(25.1%)</td>
                </tr>
                <tr>
                <td>2020</td>
                <td>5865</td>
                <td>26 paper award nominees (0.44%) + 335 oral (5.7%) </td>
                <td>1135(19.3%)</td>
                <td>1467(25.0%)</td>
                </tr>
                <tr>
                <td>2021</td>
                <td>7039</td>
                <td>
                <p>32 best paper candidates (0.45%) + 295 oral (4.19%)</p>
                <p>1047 withdrawn (14.87%)</p>
                </td>
                <td>1366(19.4%)</td>
                <td>1663(23.6%)</td>
                </tr>
                <tr>
                <td>2022</td>
                <td>8161</td>
                <td> </td>
                <td> </td>
                <td> </td>
                </tr>
              </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>ICCV (1987–) (CCF A类)  3月 h5: 129->176->184</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>oral+spotlight</th>
            <th>poster</th>
            <th>total</th>
            </tr>
            </thead>
            <tbody>
              <tr>
              <td> 1988</td>
              <td>306</td>
              <td></td>
              <td></td>
              <td> 89(29.1%)</td>
              </tr>
              <tr>
              <td> 1993</td>
              <td>380</td>
              <td></td>
              <td></td>
              <td> 101(26.6%)</td>
              </tr>
              <tr>
              <td> 1998</td>
              <td>550</td>
              <td></td>
              <td></td>
              <td> 167(30.4%)</td>
              </tr>
              <tr>
              <td> 1999</td>
              <td>575</td>
              <td></td>
              <td></td>
              <td> 178(31.0%)</td>
              </tr>
              <tr>
              <td> 2001</td>
              <td>596</td>
              <td> 45(7.6%)</td>
              <td> 160(26.8%)</td>
              <td> 205(34.4%)</td>
              </tr>
              <tr>
              <td> 2003</td>
              <td>966</td>
              <td> 43(4.4%)</td>
              <td> 155(16.1%)</td>
              <td> 198(20.5%)</td>
              </tr>
              <tr>
              <td> 2005</td>
              <td>1230</td>
              <td> 46(3.7%)</td>
              <td> 198(16.2%)</td>
              <td> 244(19.8%)</td>
              </tr>
              <tr>
              <td> 2007</td>
              <td>1190</td>
              <td> 47(3.9%)</td>
              <td> 227(19.1%)</td>
              <td> 274(23.0%)</td>
              </tr>
              <tr>
              <td> 2009</td>
              <td>1327</td>
              <td> 48(3.6%)</td>
              <td> 260(19.6%)</td>
              <td> 308(23.2%)</td>
              </tr>
              <tr>
              <td> 2011</td>
              <td>1433</td>
              <td> 39(2.7%)</td>
              <td> 394(27.5%)</td>
              <td> 433(30.2%)</td>
              </tr>
              <tr>
              <td> 2013</td>
              <td>1629</td>
              <td> 41(2.5%)</td>
              <td> 413(25.4%)</td>
              <td> 454(27.9%)</td>
              </tr>
              <tr>
              <td> 2015</td>
              <td>1698</td>
              <td> 56(3.3%)</td>
              <td> 469(27.6%)</td>
              <td> 525(30.9%)</td>
              </tr>
              <tr>
              <td> 2017</td>
              <td>2143</td>
              <td> 56(2.6%)+56(2.6%)</td>
              <td> 509(23.7%)</td>
              <td> 621(28.9%)</td>
              </tr>
              <tr>
              <td> 2019</td>
              <td>4303</td>
              <td>200(4.6%)-oral</td>
              <td> 875(20.3%)</td>
              <td> 1075(25.0%)</td>
              </tr>
              <tr>
              <td> 2021</td>
              <td>6236</td>
              <td> 210(3.4%)-oral</td>
              <td> 1407(22.6%)</td>
              <td> 1617(25.9%)</td>
              </tr>
              <tr>
              <td> 2023</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>ECCV (1990–)（CCF B类） 3月 h5: 137->144->197</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>oral+spotlight</th>
            <th>poster</th>
            <th>total</th>
              </tr>
              </thead>
              <tbody>
              <td>2002</td>
              <td>600</td>
              <td>45(7.5%)</td>
              <td>181(30.2%)</td>
              <td>226(37.7%)</td>
              </tr>
              <tr>
              <td>2004</td>
              <td>555</td>
              <td>41(7.4%)</td>
              <td>149(26.8%)</td>
              <td>190(34.2%)</td>
              </tr>
              <tr>
              <td>2006</td>
              <td>900</td>
              <td>40(4.4%)</td>
              <td>153(17.0%)</td>
              <td>193(21.4%)</td>
              </tr>
              <tr>
              <td>2008</td>
              <td>871</td>
              <td>40(4.6%)</td>
              <td>203(23.3%)</td>
              <td>243(27.9%)</td>
              </tr>
              <tr>
              <td>2010</td>
              <td>1174</td>
              <td>39(3.3%)</td>
              <td>286(24.4%)</td>
              <td>325(27.7%)</td>
              </tr>
              <tr>
              <td>2012</td>
              <td>1111</td>
              <td>40(3.6%)</td>
              <td>238(21.4%)</td>
              <td>278(25.0%)</td>
              </tr>
              <tr>
              <td>2014</td>
              <td>1359</td>
              <td>38(2.8%)</td>
              <td>325(25.1%)</td>
              <td>363(27.9%)</td>
              </tr>
              <tr>
              <td>2016</td>
              <td>1561</td>
              <td>28(1.8%)+45(3.7%)</td>
              <td>342(22.9%)</td>
              <td>415(26.6%)</td>
              </tr>
              <tr>
              <td>2018</td>
              <td>2439</td>
              <td>59(2.4%)</td>
              <td>719(29.4%)</td>
              <td>778(31.8%)</td>
              </tr>
              <tr>
              <td>2020</td>
              <td>5150</td>
              <td>104(2%)+160(3%)</td>
              <td>1096(21.3%)</td>
              <td>1360(26%)</td>
              </tr>
              <tr>
              <td>2022</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>ICML (1980–) (CCF A类)  2月 h5: 135->171->204</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>oral</th>
            <th>poster</th>
            <th>total</th>
            </tr>
            </thead>
            <tbody>
              <tr>
              <td>1999</td>
              <td>152</td>
              <td> </td>
              <td> </td>
              <td>54(35.5%)</td>
              </tr>
              <tr>
              <td>2000</td>
              <td>349</td>
              <td> </td>
              <td> </td>
              <td>151(43.3%)</td>
              </tr>
              <tr>
              <td>2001</td>
              <td>249</td>
              <td> </td>
              <td> </td>
              <td>80(32.1%)</td>
              </tr>
              <tr>
              <td>2002</td>
              <td>261</td>
              <td> </td>
              <td> </td>
              <td>86(33.0%)</td>
              </tr>
              <tr>
              <td>2003</td>
              <td>371</td>
              <td> </td>
              <td> </td>
              <td>119(32.1%)</td>
              </tr>
              <tr>
              <td>2004</td>
              <td>368</td>
              <td> </td>
              <td> </td>
              <td>118(32.1%)</td>
              </tr>
              <tr>
              <td>2005</td>
              <td>491</td>
              <td> </td>
              <td> </td>
              <td>134(27.3%)</td>
              </tr>
              <tr>
              <td>2006</td>
              <td>700</td>
              <td> </td>
              <td> </td>
              <td>140(20.0%)</td>
              </tr>
              <tr>
              <td>2007</td>
              <td>522</td>
              <td> </td>
              <td> </td>
              <td>150(28.7%)</td>
              </tr>
              <tr>
              <td>2008</td>
              <td>583</td>
              <td> </td>
              <td> </td>
              <td>155(26.6%)</td>
              </tr>
              <tr>
              <td>2009</td>
              <td>595</td>
              <td> </td>
              <td> </td>
              <td>160(26.9%)</td>
              </tr>
              <tr>
              <td>2010</td>
              <td>594</td>
              <td> </td>
              <td> </td>
              <td>152(25.6%)</td>
              </tr>
              <tr>
              <td>2011</td>
              <td>589</td>
              <td> </td>
              <td> </td>
              <td>152(25.8%)</td>
              </tr>
              <tr>
              <td>2012</td>
              <td>890</td>
              <td> </td>
              <td> </td>
              <td>242(27.2%)</td>
              </tr>
              <tr>
              <td>2015</td>
              <td>1037</td>
              <td> </td>
              <td> </td>
              <td>270(26.0%)</td>
              </tr>
              <tr>
              <td>2016</td>
              <td>1342</td>
              <td> </td>
              <td> </td>
              <td>322(24%)</td>
              </tr>
              <tr>
              <td>2017</td>
              <td>1676</td>
              <td> </td>
              <td> </td>
              <td>434(25.9%)</td>
              </tr>
              <tr>
              <td>2018</td>
              <td>2473</td>
              <td> </td>
              <td> </td>
              <td>621(25.1% )</td>
              </tr>
              <tr>
              <td>2019</td>
              <td>3424</td>
              <td> </td>
              <td> </td>
              <td>774(22.6%)</td>
              </tr>
              <tr>
              <td>2020</td>
              <td>4990</td>
              <td> </td>
              <td> </td>
              <td>1088(21.8%)</td>
              </tr>
              <tr>
              <td>2021</td>
              <td>5513</td>
              <td>
              <p>Outstanding-4 (0.07%)</p>
              <p>Long-166 (3.0%) </p>
              </td>
              <td>1017 (18.46%) short</td>
              <td>1184(21.4%)</td>
              </tr>
              <tr>
              <td>2022</td>
              <td> </td>
              <td>Long-xxx (xx%)</td>
              <td>Short-xxx (xx%)</td>
              <td>xxx (xxx%)</td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>NeurIPS (1987–) (CCF A类)  5月 h5: 169->198->245</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>oral+spotlight</th>
            <th>poster</th>
            <th>total</th>
            </tr>
            </thead>
            <tbody>
              <tr>
              <td>2001</td>
              <td>650</td>
              <td> </td>
              <td> </td>
              <td>196(30.2%)</td>
              </tr>
              <tr>
              <td>2002</td>
              <td>710</td>
              <td> </td>
              <td> </td>
              <td>221(31.1%)</td>
              </tr>
              <tr>
              <td>2003</td>
              <td>717</td>
              <td> </td>
              <td> </td>
              <td>198(27.6%)</td>
              </tr>
              <tr>
              <td>2004</td>
              <td>690</td>
              <td> </td>
              <td> </td>
              <td>207(30%)</td>
              </tr>
              <tr>
              <td>2005</td>
              <td>822</td>
              <td> </td>
              <td> </td>
              <td>207(25.0%)</td>
              </tr>
              <tr>
              <td>2006</td>
              <td>833</td>
              <td> </td>
              <td> </td>
              <td>204(24.0%)</td>
              </tr>
              <tr>
              <td>2007</td>
              <td>904</td>
              <td> </td>
              <td> </td>
              <td>217(24.0%)</td>
              </tr>
              <tr>
              <td>2008</td>
              <td>1022</td>
              <td> </td>
              <td> </td>
              <td>250(24.5%)</td>
              </tr>
              <tr>
              <td>2009</td>
              <td>1105</td>
              <td> </td>
              <td> </td>
              <td>263(23.8%)</td>
              </tr>
              <tr>
              <td>2010</td>
              <td>1219</td>
              <td> </td>
              <td> </td>
              <td>293(24.0%)</td>
              </tr>
              <tr>
              <td>2011</td>
              <td>1400</td>
              <td> </td>
              <td> </td>
              <td>308(22.0%)</td>
              </tr>
              <tr>
              <td>2012</td>
              <td>1467</td>
              <td> </td>
              <td> </td>
              <td>370(25.2%)</td>
              </tr>
              <tr>
              <td>2013</td>
              <td>1420</td>
              <td> </td>
              <td> </td>
              <td>359(25.3%)</td>
              </tr>
              <tr>
              <td>2014</td>
              <td> 1678</td>
              <td> </td>
              <td> </td>
              <td>414(24.7%)</td>
              </tr>
              <tr>
              <td>2015</td>
              <td> 1838</td>
              <td> </td>
              <td> </td>
              <td>403(21.0%)</td>
              </tr>
              <tr>
              <td>2016</td>
              <td> 2515</td>
              <td> </td>
              <td> </td>
              <td>571(22.7%)</td>
              </tr>
              <tr>
              <td>2017</td>
              <td> 3240</td>
              <td>40(1.2%)+112(3.5%)</td>
              <td>526(16.2%)</td>
              <td>678(20.9%)</td>
              </tr>
              <tr>
              <td>2018</td>
              <td> 4856</td>
              <td>30(0.6%)+168(3.5%)</td>
              <td>813(16.7%)</td>
              <td>1011(20.8%)</td>
              </tr>
              <tr>
              <td>2019</td>
              <td> 6743</td>
              <td>36(0.5%)+164(2.4%)</td>
              <td>1228(18.2%)</td>
              <td>1428(21.1%)</td>
              </tr>
              <tr>
              <td>2020</td>
              <td> 9454</td>
              <td>105(1.1%)+280(2.96%)</td>
              <td>1515(16.0%)</td>
              <td>1900(20.1%)</td>
              </tr>
              <tr>
              <td>2021</td>
              <td> 9122</td>
              <td>56(0.6%)+282(3.1%)</td>
              <td>1996(21.9%)</td>
              <td>2334(25.5%)</td>
              </tr>
              <tr>
              <td>2022</td>
              <td> </td>
              <td> </td>
              <td> </td>
              <td> </td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>AAAI (1980–) (CCF A类)  8-9月 h5: 95->126->157</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>Accepted</th>
            <th>total</th>
            </tr>
            </thead>
            <tbody>
              <tr>
              <td>2022</td>
              <td>
              <p>9251-submission</p>
              <p>9020-reviewed</p>
              <p>xxx-Phase II</p>
              </td>
              <td>1349</td>
              <td>14.96%</td>
              </tr>
              <tr>
              <td>2021</td>
              <td>
              <p>9034-submission</p>
              <p>7911-reviewed</p>
              <p>4250-Phase II</p>
              </td>
              <td>1692</td>
              <td>21.0%</td>
              </tr>
              <tr>
              <td>2020</td>
              <td>8800</td>
              <td>1591</td>
              <td>20.6%</td>
              </tr>
              <tr>
              <td>2019</td>
              <td>7700</td>
              <td>1150</td>
              <td>16.2%</td>
              </tr>
              <tr>
              <td>2018</td>
              <td>3808</td>
              <td>983</td>
              <td>24.6%</td>
              </tr>
              <tr>
              <td>2017</td>
              <td>2590</td>
              <td>638</td>
              <td>24.6%</td>
              </tr>
              <tr>
              <td>2016</td>
              <td>2132</td>
              <td>549</td>
              <td>25.8%</td>
              </tr>
              <tr>
              <td>2015</td>
              <td>1991</td>
              <td>531</td>
              <td>26.7%</td>
              </tr>
              <tr>
              <td>2014</td>
              <td>1406</td>
              <td>398</td>
              <td>28.0%</td>
              </tr>
              <tr>
              <td>2013</td>
              <td>934?</td>
              <td>271?</td>
              <td>29.0%</td>
              </tr>
              <tr>
              <td>2012</td>
              <td>1129</td>
              <td>294</td>
              <td>26.0%</td>
              </tr>
              <tr>
              <td>2011</td>
              <td>975</td>
              <td>242</td>
              <td>24.8%</td>
              </tr>
              <tr>
              <td>2010</td>
              <td>982</td>
              <td>264</td>
              <td>26.9%</td>
              </tr>
              <tr>
              <td>2008</td>
              <td>937</td>
              <td>227</td>
              <td>24.2%</td>
              </tr>
              <tr>
              <td>2007</td>
              <td>923</td>
              <td>251</td>
              <td>27.0%</td>
              </tr>
              <tr>
              <td>2006</td>
              <td>776</td>
              <td>236</td>
              <td>30.0%</td>
              </tr>
              <tr>
              <td>2005</td>
              <td>803</td>
              <td>225</td>
              <td>28.5%</td>
              </tr>
              <tr>
              <td>2004</td>
              <td>453</td>
              <td>121</td>
              <td>26.7%</td>
              </tr>
              <tr>
              <td>2002</td>
              <td>469</td>
              <td>121</td>
              <td>25.8%</td>
              </tr>
              <tr>
              <td>2000</td>
              <td>431</td>
              <td>143</td>
              <td>33.2%</td>
              </tr>
              <tr>
              <td>1999</td>
              <td>400</td>
              <td>109</td>
              <td>27.3%</td>
              </tr>
              <tr>
              <td>1998</td>
              <td>475</td>
              <td>144</td>
              <td>30.3%</td>
              </tr>
              <tr>
              <td>1997</td>
              <td>323</td>
              <td>117</td>
              <td>36.0%</td>
              </tr>
              <tr>
              <td>1996</td>
              <td>643</td>
              <td>197</td>
              <td>30.0%</td>
              </tr>
              <tr>
              <td>1994</td>
              <td>780</td>
              <td>222</td>
              <td>28.0%</td>
              </tr>
              <tr>
              <td>1993</td>
              <td>524</td>
              <td>126</td>
              <td>24.0%</td>
              </tr>
              <tr>
              <td>1992</td>
              <td>636</td>
              <td>133</td>
              <td>21.0%</td>
              </tr>
              <tr>
              <td>1991</td>
              <td>603</td>
              <td>142</td>
              <td>24.0%</td>
              </tr>
              <tr>
              <td>1990</td>
              <td>892</td>
              <td>161</td>
              <td>18.0%</td>
              </tr>
              <tr>
              <td>1988</td>
              <td>850</td>
              <td>148</td>
              <td>17.0%</td>
              </tr>
              <tr>
              <td>1987</td>
              <td>715</td>
              <td>149</td>
              <td>21.0%</td>
              </tr>
              <tr>
              <td>1986</td>
              <td>817</td>
              <td>187</td>
              <td>23.0%</td>
              </tr>
            </tbody>
            </table>
          </p>
        </li>
        <li>
          <p>
            <b>IJCAI (1969–) (CCF A类)  2月 h5: h5: 67->95->105</b><br><br>
            <table border="1" cellspacing="0" width="300">
            <thead>
            <tr>
            <th>year</th>
            <th>submitted</th>
            <th>accepted</th>
            <th>total</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>2022</td>
            <td>
            <p>xxx-abstract</p>
            <p>xxx-full paper</p>
            <p>xxx-desk rejected</p>
            <p>xxx-summary rejected</p>
            <p>xxx-Phase II</p>
            </td>
            <td>xxx</td>
            <td>xxx%</td>
            </tr>
            <tr>
            <td>2021</td>
            <td>
            <p>5534-abstract</p>
            <p>4204-full paper</p>
            <p>144-desk rejected</p>
            <p>1002-summary rejected</p>
            <p>3033-Phase II</p>
            </td>
            <td>587</td>
            <td>13.9%</td>
            </tr>
            <tr>
            <td>2020</td>
            <td>4717</td>
            <td>592</td>
            <td>12.60%</td>
            </tr>
            <tr>
            <td>2019</td>
            <td>4752</td>
            <td>850</td>
            <td>17.88%</td>
            </tr>
            <tr>
            <td>2018</td>
            <td>3470</td>
            <td>710</td>
            <td>20.5%</td>
            </tr>
            <tr>
            <td>2016</td>
            <td>2294</td>
            <td>551</td>
            <td>24.0%</td>
            </tr>
            <tr>
            <td>2015</td>
            <td>1996</td>
            <td>572</td>
            <td>28.7%</td>
            </tr>
            <tr>
            <td>2013</td>
            <td>1473</td>
            <td>413</td>
            <td>28.0%</td>
            </tr>
            <tr>
            <td>2011</td>
            <td>1325</td>
            <td>400</td>
            <td>30.2%</td>
            </tr>
            <tr>
            <td>2009</td>
            <td>1290</td>
            <td>331</td>
            <td>25.7%</td>
            </tr>
            <tr>
            <td>2007</td>
            <td>1353</td>
            <td>212</td>
            <td>15.7%</td>
            </tr>
            <tr>
            <td>2005</td>
            <td>1329</td>
            <td>240</td>
            <td>18.1%</td>
            </tr>
            <tr>
            <td>2003</td>
            <td>913</td>
            <td>189</td>
            <td>20.7%</td>
            </tr>
            <tr>
            <td>2001</td>
            <td>796</td>
            <td>197</td>
            <td>24.7%</td>
            </tr>
            <tr>
            <td>1999</td>
            <td>750</td>
            <td>194</td>
            <td>25.9%</td>
            </tr>
            <tr>
            <td>1997</td>
            <td>882</td>
            <td>216</td>
            <td>24.5%</td>
            </tr>
            <tr>
            <td>1995</td>
            <td>1112</td>
            <td>249</td>
            <td>22.4%</td>
            </tr>
            </tbody>
            </table>
          </p>
        </li>
      </ul>
  
      <div id="footer">
        <div id="footer-text"></div>
      </div>
      © Deng-Ping Fan

      <div class="container">
         <div style="display:inline-block;width:200px;">
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5d4snb3frum&amp;s=220&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
        </div>
      </div>

    </div>
  </body>
</html>
